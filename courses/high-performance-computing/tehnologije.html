<!DOCTYPE html>
<html>

<head>
	<title>[High-performance computing (HPC)](/courses/#table-of-contents)</title>
	<meta charset="utf-8">
	<link rel="stylesheet" href="../remarkslides.css">
	<!-- MathJaxâ„¢ -->
	<script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML" async>
	</script>
	<!-- mermaid dijagram -->
	<link rel="stylesheet2" href="../mermaid.min.css">
	<script>
		mermaid.initialize({startOnLoad:true});
	</script>
	<!-- Global site tag (gtag.js) - Google Analytics -->
	<script async src="https://www.googletagmanager.com/gtag/js?id=UA-127734928-1"></script>
	<script>
		window.dataLayer = window.dataLayer || [];
		      function gtag(){dataLayer.push(arguments);}
		      gtag('js', new Date());
		
		      gtag('config', 'UA-127734928-1');
	</script>
	<!-- google analytics -->
	<script>
		(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
		      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
		      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
		      })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
		      ga('create', 'UA-127734928-1', 'auto');
		      ga('send', 'pageview');
	</script>
</head>

<body>
	<textarea id="source">class: center, middle

## [High-performance computing (HPC)](/courses/#table-of-contents)
### Tehnologije


.author[[dr. Milovan TomaÅ¡eviÄ‡](https://www.milovantomasevic.com/resume/)]

.small[[Fakulteta za informacijske Å¡tudije v Novem mestu (FIÅ )](https://www.fis.unm.si/en/)</br>![:scale 10%](../fis/fis.png) .small[ [ğŸŒâ™ milovan.tomasevic.fis.unm.si](http://milovan.tomasevic.fis.unm.si)</br> [ğŸ“§â™ milovan.tomasevic@fis.unm.si](mailto:milovan.tomasevic@fis.unm.si)]]



.created[08.03.2019 u 16:49]


---


class: center, middle, inverse

# Tehnologije Paralelnog Programiranja
#### Å¡ta je na raspolaganju ?

---

name: sadrzaj

# SadrÅ¾aj

- [OpenMP](#openmp)
- [OpenMPI](#openmpi)
- [Koncept akceleratora i akceleratorske arhitekture](#kaaa)
- [OpenACC](#openacc)

---
name: openmp
class: center, middle, inverse

# OpenMP
#### model deljenje memorije

---
layout: true

.section[[OpenMP](#sadrzaj)]

---

## Å ta je OpenMP?

- Ovo bi sve trebali da veÄ‡ znate.
- No, dovoljno je bitno da preÄ‘emo ponovo.
- OpenMP je `Open MultiProcessing`

--
- Centralna ideja je deljena memorija i viÅ¡e niti izvrÅ¡avanja
- Nekada davno, svo paralelno programiranje je bilo ovako.
- Ovo je i dalje stil programiranja koji je najÄeÅ¡Ä‡i u korisniÄkim aplikacijama zato Å¡to se izvrÅ¡avaju na jednoj maÅ¡ini, bez obzira koliko procesora ima.
- Sam jezik je C/C++ mada OpenMP postoji i za Fortran.

---

## Arhitektura OpenMP-a

- Glavna jedinica podele izvrÅ¡avanja u OpenMP je nit (*thread*)
- Niti se mogu podeliti u:
    - Glavnu
    - Niti radilice (*worker threads*)
- Svaka nit je nezavisna traka izvrÅ¡avanja koja prolazi kroz program
- Ono Å¡to odlikuje glavnu nit jeste Å¡to poÄinje prva i Å¡to njeno zavrÅ¡avanje zavrÅ¡ava program. Glavna nit takoÄ‘e pokreÄ‡e, zaustavlja i kontroliÅ¡e niti-radilice.
- NaÄin na koji se ovakvo ponaÅ¡anje postiÅ¾e jeste kroz OpenMP direktive.

---

## Koliko niti?

- Koliko mi hoÄ‡emo.
- U praksi, broj niti jako zavisi od naÅ¡e svrhe. Ako imamo viÅ¡e niti da bi ustvari efektno Äekali na viÅ¡e stvari istovremeno zato Å¡to smo u sistemu koji je I/O-ograniÄen onda je broj niti neograniÄen, odnosno, samo ograniÄen sposobnoÅ¡Ä‡u sistema da to istrpi.
- Ovako rade sistemi, npr. web servera ili sistemi koji rade sa velikim brojem operacija nad fajlovima.
- Ovako, takoÄ‘e, rade sistemi koji hoÄ‡e da pruÅ¾e glatko interaktivno iskustvo dok neÅ¡to rade u pozadini: onda obiÄno postoji jedna GUI nit i viÅ¡e niti-radilica.
- Neki visoko interaktivni sistemi dodeljuju niti zadacima. TipiÄan primer su video igre gde obiÄno imate podelu tipa: nit za iscrtavanje, nit za kontrolu, nit za zvuk i nit za veÅ¡taÄku inteligenciju, u zavisnosti od prirode engine-a koji se koristi.

---

## Koliko niti?

- Ovde, naÅ¡ cilj je maksimum brzine stoga je pravilo mnogo jednostavnije.
- Broj OpenMP niti treba da bude manji ili jednak ukupnom broju sistemskih niti na raspolaganju.
- Sistemske niti su ravne broju `procesora` u kontekstu SLURM-a, tj. broju stvari koje procesor(i) u sistemu mogu da rade istovremeno.
- Å to ne viÅ¡e? 
- Zato Å¡to se paralelizam preko hardverske granice ostvaruje koristeÄ‡i preemptivno izvrÅ¡avanje Å¡to znaÄi da umesto da dobijamo na ukupnoj ostvarenoj brzini mi je gubimo na *context-switching overhead.*

---

## Fork-join model izvrÅ¡avanja

![:scale 75%](img/Fork_join.png)

---

## Fork-Join model izvrÅ¡avanja

- Program poÄinje izvrÅ¡avajuÄ‡i samo jednu stvar, sekvencijalno.
- U nekim trenucima, stvara se veÄ‡i broj dodatnih niti izvrÅ¡avanja (`raÄvanje`).
- Te niti izvrÅ¡avanja se izvode nezavisno sve dok se ne prikljuÄe ponovo glavnoj niti kroz formu implicitne sinhronizacije barijerom.

---

## SPMD

- NajÄeÅ¡Ä‡a forma koji fork/join paralelizam ima u okviru OpenMP-a jeste *Single Program Multiple Data*, praktiÄni roÄ‘ak SIMD arhitekture koji smo pominjali pre par Äasova.
- To znaÄi da svaka od niti izvrÅ¡ava isti kod koga samo razlikuju za nit specifiÄne privatne promenljive (viÅ¡e o ovome kasnije).
- Ovo je onda naÄin da podelimo posao na iste komade.

---

## Alternative SPMD

- OpenMP je jednako lako konfigurisati da paralelno izvrÅ¡ava razliÄite komade koda.
- Ponekad algoritam ovo zahteva, mada to nije nuÅ¾no dobra ideja.
- Heterogen paralelni kod se ne uklapa tako glatko u fork/join arhitekturu zbog toga Å¡to je vreme izvrÅ¡avanja nepredvidivo Å¡to znaÄi da join moÅ¾e da ima nepredvidivu koliÄinu Äekanja da se izvrÅ¡avanje sinhronizuje.
- Najbolje je ovo prilagoditi kontekstu algoritma.

---

## UgnjeÅ¾den paralelizam

- MoguÄ‡e je raÄvati nit izvrÅ¡avanja unutar veÄ‡ raÄvane niti izvrÅ¡avanja.
- Tj. fork unutar fork-a.
- Kako se ovo izvrÅ¡i zavisi od toga kako je OpenMP implementacija koja se koristi implementirana.
- Neke ignoriÅ¡u dublje slojeve paralelizacije i tretiraju ih kao sekvencijalan kod, a neki izvrÅ¡avaju kako je napisano dok god ima neiskoriÅ¡Ä‡enih sistemskih niti.
- Ovakva forma ugnjeÅ¾davanja je korisna ili u specijalizovanim algoritmima ili kada imamo raÄunar sa jako puno paralelnih niti.
- Intel planira da napravi procesor sa 28 jezgara i 56 sistemskih niti: moÅ¾da nekakav hipotetiÄki Ävor sa Äetiri takva procesora.

---

## Promenljive niti

- OpenMP je baziran na modelu deljene memorije.
- Podrazumevano je da sve promenljive budu deljene tj. da svaka nit moÅ¾e da im pristupi.
- OdreÄ‘ene promenljive, sa druge strane, mogu da budu podeljene tako da svaka nit ima svoj primerak.
- Ovo bi moglo da se uradi i ruÄno, tako Å¡to imamo nekakvu strukturu podatka gde se promenljive indeksiraju kroz broj niti, ali OpenMP to omoguÄ‡ava automatski.
- Neke promenljive nisu ni privatne ni globalne, doduÅ¡e, no meÅ¡aju ta dva pristupa. Ovo je od koristi kada se rade operacije redukcije, o Äemu viÅ¡e kasnije.

---

## Sistemske promenljive i OpenMP

- OpenMP gleda vrednosti sistemskih promenljivih ne bi li odredio svoje ponaÅ¡anje.
- Sistemske promenljive mogu biti nameÅ¡tene:
    - U profilu korisnika.
    - U skripti koja pokreÄ‡e aplikaciju.
    - Na komandnoj liniji.

---

## Sistemske promenljive koje poznaje OpenMP

.center-table.small[

|  **Promenljiva**  | **Tip vrednosti** |                                                                          **ZnaÄenje**                                                                          |
|:-----------------:|:-----------------:|:--------------------------------------------------------------------------------------------------------------------------------------------------------------:|
| `OMP_NUM_THREADS` |        Broj       | Broj istovremenih niti                                                                                                                                         |
|   `OMP_DYNAMIC`   |       Bulova      | DinamiÄki menja broj istovremenih niti. MoÅ¾e poveÄ‡ati efikasnost kroz adaptaciju, ali ima cenu u performansama.                                                |
|   `OMP_SCHEDULE`  |      ReÄ.Broj     | Tip rasporeda izvrÅ¡avanja praÄ‡en dimenzijom particije izvrÅ¡avanja.  ViÅ¡e o tome kasnije.                                                                       |
|    `OMP_NESTED`   |       Bulova      | Da li imamo ugnjeÅ¾deni paralelizam ili ne. MoÅ¾e da nam pomogne da upravljamo distribucijom niti u komplikovanim situacijama, ali ima fiksnu cenu u overhead-u. |
|    `OMP_CANCELLATION`   |             Bulova             | Da li 'cancel' direktiva radi ili ne.                                          |
| `OMP_MAX_ACTIVE_LEVELS` |              Broj              | Koliko ugnjeÅ¾denih regiona je dozvoljeno. Podrazumevano je da je neograniÄeno. |
|`OMP_MAX_TASK_PRIORITY`  |              Broj              | NajveÄ‡i sistemski prioritet koji se dodeljuje zadacima.                        |
|     `OMP_STACKSIZE`     | Broj praÄ‡en  sa B, K, M, ili G | VeliÄina sistemskog steka jednu nit izvrÅ¡avanja                                |
]

---

## BiblioteÄke rutine

- OpenMP se sastoji od: sistemskog okruÅ¾enja izvrÅ¡avanja, biblioteka, i direktiva.
- Biblioteke su klasiÄne C biblioteke i nude funkcionalnost aplikacije kroz funkcije.
- Fajl zaglavlja za C biblioteku je `omp.h`

```c
int n = omp_get_num_threads(); 
//primetite stil imena
//peefiks je omp, a reÄi su razdvojene sa _ umesto kroz camelCase
//ova funkcija vraÄ‡a tekuÄ‡i broj niti koji, prirodno, nikada neÄ‡e
//biti veÄ‡i od najveÄ‡eg broja niti koji smo podesili.

int k = omp_get_thread_num(); 
//terminologija moÅ¾e biti
//malo konfuzna ali ono Å¡to ovo radi jeste vraÄ‡a broj tekuÄ‡e niti ou opsegu od 0 do n - 1. 
// Glavna nit uvek dobija vrednost 0, Å¡to nam omoguÄ‡ava da je detektujemo.
```

---

## ÄŒemu direktive?

- OpenMP ima nezavidan zadatak
- C je fundamentalno napravljen sa idejom jednostrukog izvrÅ¡avanja
- To je ugraÄ‘eno u sam jezik
- VeÄ‡ina sistema za paralelnog izvrÅ¡avanje radi tako Å¡to nas tera da eksplicitno pravimo strukture podataka koje predstavljaju jedinice paralelnog izvrÅ¡avanja, te ih pokreÄ‡emo sami.
- Ovo Äini takav kod izuzetno nezgodnim za praÄ‡enje.
- RazmiÅ¡ljajte o, recimo, Pthread tehnologiji

---

## Pthreads

.lcol[

```c
#include <pthread.h>
#include <stdio.h>

/* this function is run by the second thread */
void *inc_x(void *x_void_ptr)
{

    /* increment x to 100 */
    int *x_ptr = (int *)x_void_ptr;
    while(++(*x_ptr) < 100);

    printf("x increment finished\n");

    /* the function must return something - NULL will do */
    return NULL;

}
``` 
]

.rcol[

- Morali smo da paralelni kod enkapsuliramo u svojoj funkciji koja, zbog univerzalnosti i Å¡alje i vraÄ‡a `void*` pokazivaÄe.
- Ovo je uÅ¾asavajuÄ‡e sa taÄke glediÅ¡ta dobre prakse kodiranja. Plus, kod sada nije tamo gde se izvrÅ¡ava nego gurnut u funkciju koja niÅ¡ta u stvari ne vraÄ‡a no menja okruÅ¾enje iz koga je pozvana.

]

.footer.medium[
  [Detaljnije](https://timmurphy.org/2010/05/04/pthreads-in-c-a-minimal-working-example/)
] 

---

## Pthreads

.lcol.small[

```c
int main()
{

    int x = 0, y = 0;

    /* show the initial values of x and y */
    printf("x: %d, y: %d\n", x, y);

    /* this variable is our reference to the second thread */
    pthread_t inc_x_thread;

    /* create a second thread which executes inc_x(&x) */
    if(pthread_create(&inc_x_thread, NULL, inc_x, &x)) {

        fprintf(stderr, "Error creating thread\n");
        return 1;

    }
    /* increment y to 100 in the first thread */
    while(++y < 100);

    printf("y increment finished\n");

    /* wait for the second thread to finish */
    if(pthread_join(inc_x_thread, NULL)) {

        fprintf(stderr, "Error joining thread\n");
        return 2;

    }

    /* show the results - x is now 100 thanks to the second thread */
    printf("x: %d, y: %d\n", x, y);

    return 0;

}
```
]

.rcol[

- Primetite da za sve koristimo niti koje identifikuje `pthread_t` tip. Ovo je zato Å¡to novu paradigmu programiranja moramo da "proteramo" kroz kod koji za to nije namenjen.

```console
# This code will print something like:

x: 0, y:0
y increment finished
x increment finished
x: 100, y: 100
```

]

---

## Å ta je alternativa?

- Pa i nema je baÅ¡.
- Problem je u tome Å¡to C nije napravljen da bude proÅ¡iriv.
- Ako konstrukti ugraÄ‘eni u jezik ne zadovoljavaju naÅ¡ obrazac programiranja nema mnogo toga Å¡to moÅ¾emo da uradimo.
- MoÅ¾emo da koristimo pre-procesor da modifikujemo kod pre negonstigne kod kompajlera.
- Ovakav pristup vrlo uspeÅ¡no koristi, npr. Qt koji dodaje konstrukte u C++ koji prvobitno nisu bili tu.
- Problem sa tim jeste Å¡to kod koji se zaista izvrÅ¡ava (i kod koji posle moramo da debagujemo) nema baÅ¡ puno veze sa onim Å¡to smo napisali. Ovo je generator glavobolja.

---

## FOSS reÅ¡enje

- GCC je open source.
- `git clone git@github.com:gcc-mirror/gcc.git` i veÄ‡ se bavimo razvojem kompajlera.
- ZaÅ¡to jednostavno ne proÅ¡iriti C funkcionalnoÅ¡Ä‡u koja nam treba?
- Zato Å¡to to onda nije viÅ¡e standardni C.
- Suptilne nekompatibilnosti izmeÄ‘u standarda i implementacije su odgovorne za neverovatne komplikacije. Plus, naÅ¡ kod je sada zauvek vezan za tu, modifikovanu verziju GCC-a.
- MoÅ¾e li bolje?

---

## `#pragma`

- `#pragma` je jednostavna ideja
- To je naÄin da se direktno obratimo kompajleru i damo nekakve instrukcije.
- `#pragma` direktive su eksplicitno tu da budu nestandardne: svaki kompajler je potpuno slobodan da doda bilo koji broj svojih pragmi koje rade Å¡ta god taj kompajler hoÄ‡e.
- â€¦sve dok ignoriÅ¡u sve pragme koje ne znaju Å¡ta rade.
- Ovo je tajni sastojak koji omoguÄ‡ava proÅ¡irivost. NaÅ¡ kod je i dalje legalan Äak i ako je pun pragmi: kompajler koji ih ne podrÅ¾ava Ä‡e samo da ih ignoriÅ¡e i sve Ä‡e i dalje da radi.

---

## gcc i openMP podrÅ¡ka

- Da li moramo onda da modifikujemo GCC?
- Naravno da ne, neko je veÄ‡ bio fin i to uradio umesto nas.
- Sve Å¡to je neophodno jeste da kompajliramo naÅ¡ kod sa opcijom fopenmp i dobijemo svu OpenMP podrÅ¡ku koja nam treba, a biblioteÄke funkcije pruÅ¾a libgomp

---

## GCC podrÅ¡ka po kompajlerima

.center-table.small[

| **Od GCC verzije** | **PodrÅ¾an OpenMP standard** | **Na jezicima** |
|:------------------:|:---------------------------:|:---------------:|
|        4.2.0       |             2.5             |  C/C++/Fortran  |
|        4.4.0       |             3.0             |  C/C++/Fortran  |
|        4.7.0       |             3.1             |  C/C++/Fortran  |
|        4.9.0       |             4.0             |      C/C++      |
|        4.9.1       |             4.0             |  C/C++/Fortran  |
|         6.1        |             4.5             |      C/C++      |

]

---

## Najosnovniji OpenMP program

```c
#include <stdio.h>
#include "omp.h"

int main() {

    #pragma omp parallel
    {
        printf("Hello World\n");
    }

    return 0;
}
```

---

## Napomena: Pragme nisu, u stvari, magiÄne.

- Lako je zaboraviti da, koliko god da su zgodne za programiranje, pragme i konstrukti paralelnog programiranja koje donose, sve se to i dalje izvrÅ¡ava na istom procesoru kao i sav naÅ¡ drugi kod.
- Pre ili kasnije to postanu pozivi nad funkcijama, komande kontrole toka itd.

---

## Napomena: Pragme nisu, u stvari, magiÄne.

```c
    #pragma omp parallel
    {
        printf("Hello World\n");
    }
            â¬†
```

```c
                           â¬‡
  void subfunction (void *data)
  {
    use data;
    body;
  }

  setup data;
  GOMP_parallel_start (subfunction, &data, num_threads);
  subfunction (&data);
  GOMP_parallel_end ();

```

.footer.medium[
  [Detaljnije](https://gcc.gnu.org/onlinedocs/gcc-5.5.0/libgomp/Implementing-PARALLEL-construct.html)
] 

---

## Napomena: Pragme nisu, u stvari, magiÄne.

- Ipak, ako mi sve dobro programiramo i ako je FSF sve dobro programirao trebalo bi da ne moramo puno da mislimo o tome Å¡ta to OpenMP radi iza kulisa.
- Ne puno, ali pomalo.
- Ne valja da zaboravimo da su naÅ¡i alati alati.

---

## Osnovna sintaksa

- Svaka pragma za `OpenMP` poÄinje sa `#pragma omp`
- Zatim ide kljuÄna reÄ pragme koja definiÅ¡e Å¡ta ta pragma radi praÄ‡ena parametrima u zagradi (ako ih ima)
- MoguÄ‡e je pragme slagati u jednoj direktivi radi uÅ¡tede prostora.

---

## `#pragma parallel`

- Maksimalno raÄva izvrÅ¡avanje pred ulazak u predstojeÄ‡i izraz/blok i izvrÅ¡ava ga u onoliko niti koliko je specificirano.
- Nit se implicitno raÄva na poÄetku, a sinhronizuje na kraju.

---

## `#pragma private`

- Ova pragma definiÅ¡e niz promenljivih (definisanih u nizu u zagradama) kao privatne za nit koja ih koristi.
- Navodi se posle `parallel` direktive i odnosi se na niti tako stvorene.

---

## Paralelna for petlja

- NajÄeÅ¡Ä‡a forma paralellizacije jeste podela iteracija for petlje meÄ‘u nitima. 
- Ako je svaki ciklus petlje nezavisan, onda stepen paralelizma zavisi samo od ograniÄenja naÅ¡eg hardvera.
- Prirodno, OpenMP ima metode koje olakÅ¡avaju ovako neÅ¡to.

---

## Sekvencijalno zbrajanje nizova

```c
#include <stdio.h>
#include <stdlib.h>

int main () {
    const int N = 20;
    int nthreads, threadid, i;
    double a[N], b[N], result[N];

    for (i=0; i<N; i++){ a[i] = 1*i; b[i] = 2.0*i;}
    for (i= ;i<N;itt) {
        result[i] = a[i] + b[i];
    }
    printf ("TEST result[19] = %g\n", result[19]);

    return 0;
}

```

---

## Paralelno zbrajanje nizova

```c
#include <stdio.h>
#include <stdlib.h>
#include <opm.h>

int main () {
    const int N = 20;
    int nthreads, threadid, i;
    double a[N], b[N], result[N];

    for (i=0; i<N; i++){ a[i] = 1*i; b[i] = 2.0*i;}
    
    #pragma omp parallel
    { // fork
        #pragma omp for
        for (i= ;i<N;itt) {
            result[i] = a[i] + b[i];
        }
    } // join 

    printf ("TEST result[19] = %g\n", result[19]);
    
    return 0;
}

```

---

## Neke opservacije

- Primetite da je paralelno izvrÅ¡avanje i for petlja razliÄita stvar
- Paralelno izvrÅ¡avanje je baÅ¡ to.
- For petlja je konstrukt koji deli rad izmeÄ‘u niti.
- TakoÄ‘e, valja primetiti da je brojaÄka promenljiva automatski privatna, Å¡to je zgodno.
- Uprkos tome Å¡to su paralelizacija i for-deljenje odvojene operacije moguÄ‡e je (i poÅ¾eljno je!) da ih piÅ¡emo zajedno

---

## Paralelno zbrajanje nizova, kraÄ‡e

```c
#include <stdio.h>
#include <stdlib.h>
#include <opm.h>

int main () {
    const int N = 20;
    int nthreads, threadid, i;
    double a[N], b[N], result[N];

    for (i=0; i<N; i++){ a[i] = 1*i; b[i] = 2.0*i;}
    
    #pragma omp parallel for
    for (i= ;i<N;itt) {
        result[i] = a[i] + b[i];
    }

    printf ("TEST result[19] = %g\n", result[19]);
    
    return 0;
}

```

---

## Koliko ima niti? I koja radi na Äemu?

- Ovde se valja podsetiti `OMP_SCHEDULE` promenljive.
- Ona definiÅ¡e podrazumevano ponaÅ¡anje for direktive.
- `SCHEDULE` mehanizam ima za cilj, jednostavno, da sve indekse for petlje od `0` do `N-1` podeli na neki broj regiona koji se ne preklapaju.
- MoguÄ‡e je za svaku for petlju kontrolisati kako Ä‡e to da uradi kroz schedule direktivu koja u zagradama ima prvo kljuÄnu reÄ a zatim veliÄinu regiona.

---

## KljuÄne reÄi schedule direktive

.center-table.small[

| **KljuÄna reÄ** |                                                                                                                       **ZnaÄenje**                                                                                                                       |
|:---------------:|:--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------:|
|     `static`    | Iteracije se dele u komade veliÄine koja je specificirana, ako je specificirana, u suprotnom se veliÄina raÄuna tako Å¡to se ukupan broj iteracija podeli brojem niti. Zatim se tako definisani regioni dele meÄ‘u nitima koristeÄ‡i *round robin* pristup. |
|    `dynamic`    | Kao prethodno, ali osim na poÄetku, niti se dele u *first-come first- served* pristupu gde niti traÅ¾e joÅ¡ posla kada zavrÅ¡e rad.                                                                                                                         |
|     `guided`    | Kao prethodno, ali veliÄina regiona je fleksibilna i proporcionalna je broju nedodeljenih iteracija podeljenih sa brojem dostupnih niti uz minimum ravan podeÅ¡enoj veliÄini regiona.                                                                     |
|      `auto`     | Kompajler/runtime bira Å¡ta se izvrÅ¡ava i kada.                                                                                                                                                                                                           |
|    `runtime`    |  OmoguÄ‡ava da se schedule podesi iz koda koristeÄ‡i  <br> `void omp_set_schedule(omp_sched_t kind, int chunk_size);`                                                                                                                                          |
]

---

## Monotono i nemonotono izvrÅ¡avanje

- specifikaciju schedule direktive moÅ¾e pratiti kljuÄna reÄ `monotonic` i kljuÄna reÄ `nonmonotonic`.
- `Monotonic` znaÄi da svaka nit izvrÅ¡ava dodeljene iteracije u redosledu strogo poveÄ‡avajuÄ‡e vrednosti brojaÄa
- `Nonmonotonic` znaÄi da svaka nit izvrÅ¡ava dodeljene iteracije u proizvoljnom, nedeterministiÄkom redosledu.

---

## Alternativa SPMD modelu: sekcije

- Umesto da niti izvrÅ¡avaju fundamentalno isti kod (nad razliÄitim podacima) kroz deljenje posla u nitima alternativa su sekcije.
- Ideja je jednostavna: u okviru paralelnog regiona se napravi viÅ¡e blokova anotiranih sa `section` pragmom.
- MeÄ‘u nitima se onda ravnomerno rasporeÄ‘uju one koje rade razliÄite sekcije istovremeno i paralelni region se `join`-uje kada su sve sekcije obavljene.
- OÄigledno, redosled izvrÅ¡avanja apsolutno nije garantovan.

---

## Sekcije

```c
#include <stdio.h>
#include <stdlib.h>
#include <opm.h>

int main () {

    #pragma omp parallel
    {
        #pragma omp sections
        {
            // prvi pralelni blok 
            #pragma omp sections
            {
                // drugi pralelni blok 
            }
            #pragma omp sections
            {
                // treci pralelni blok 
            }
        }
    }

    return 0;
}
```

---

## Sinhronizacija

- OpenMP ima vrlo `labavu` komunikaciju izmeÄ‘u niti
- To je potencijalno odliÄno buduÄ‡i da omoguÄ‡ava da kod koji je zamalo isti kao sekvencijalni ima sve prednosti paralelizacije, i zaista, ako samo koristimo paralelizaciju operacija u petljama nema Å¡ta puno da se brinemo.
- Dosta programa radi ovako. Numpy na primer: cela tajna je multiprocesorsko jezgro za rad sa vektorima pisano u C-u koje se onda koristi u fundamentalno sekvencijlanim algoritmima.
- Ovo je maksimum koristi za minimum glavobolje.
- Ali Å¡ta ako hoÄ‡emo joÅ¡ brzine i fleksibilnije algoritme?

---

## Sinhronizacija

- Onda moramo kontrolisati `labavu` komunikaciju izmeÄ‘u niti.
- Treba da bude onoliko otvorena koliko moÅ¾e, naravno, ali nikako ne preko toga.
- U suprotnom imamo grozomorne probleme gde sav kod udara po istoj memoriji i ona zavrÅ¡i u nedefinisanom stanju.
- Nedefinisana stanja valja izbeÄ‡i.
```console
ERROR Attempt to parse HTML with regular expression; system returned Cthulhu
```

---

## UveÄ‡avanje promenljive kao primer haosa

- Mi napiÅ¡emo
    - Thread A: i++                       
    - Thread B: i++

- Ali dobijemo
    - A oÄita i sa vrednoÅ¡Ä‡u 7
    - B oÄita i sa vrednoÅ¡Ä‡u 7
    - B poveÄ‡a 7 na 8 i smesti ga u i
    - A poveÄ‡a 7 na 8 i smesti ga u i

- Ispovest: 
    - Zbog toga kako procesori rade, ovo ne moÅ¾e da se desi baÅ¡ ovako, ali je zgodan primer.

---

## Implicitna sinhronziacija

- Svaki paralelni blok je implicitno sinhronizovan, buduÄ‡i da sve niti moraju da saÄekaju da se sve ostale niti zavrÅ¡e pre nego se izvrÅ¡avanje nastavi.
- Ovo omoguÄ‡ava da delimo izvrÅ¡avanje na sekcije koje zavise jedna od druge.
- Centralni problem paralelizacije jeste, na kraju krajeva, to Å¡to u sekvencijalnom izvrÅ¡avanju relacija sekvence izvrÅ¡avanja i relacija zavisnosti su ista stvar.
- Paralelizacija je razdvajanje te dve relacije gde je to moguÄ‡e.
- Negde, neÅ¡to mora da se zavrÅ¡i da bi se kasniji deo koda izvrÅ¡io (nije moguÄ‡e koristiti neki podataka pre nego Å¡to je izraÄunat osim jako blizu povrÅ¡ini horizonta KoÅ¡ija rotirajuÄ‡ih crnih rupa sa Kerovom metrikom. MoÅ¾da.)

---

## Eksplicitna sinhronizacija

- MoguÄ‡e je naterati niti da se sinhronizuju kroz direktive i to:
    - Direktiva kritiÄnog regiona
    - Direktiva glavne niti
    - Direktiva barijere
    - Direktiva jednostrukog izvrÅ¡avanja

---

## Direktiva kritiÄnog regiona

- Direktiva kritiÄnog regiona (`#pragma omp critical`) definiÅ¡e blok koda kao kritiÄan.
- KritiÄan kod je takav da se u njemu u jednom trenutku moÅ¾e naÄ‡i samo jedna jedina nit.
- Ovo nas Å¡titi baÅ¡ od onog problema sa inkrementacijom promenljive, buduÄ‡i da sve Å¡to treba da uradimo jeste da se postaramo da je I++ u kritiÄnom regionu i znamo da Ä‡e cela operacija biti zavrÅ¡ena odjednom.

---

## Direktiva glavne niti

- Direktiva glavne niti (`#pragma omp master`) definiÅ¡e blok koda koji je takav da ga moÅ¾e izvrÅ¡iti samo i iskljuÄivo glavna nit.
- Svaka nit-radilica koja naleti na ovaj blok Ä‡e ga ignorisati kao da nije tu.
- To znaÄi da ovakav kod za razliku od veÄ‡ine sinhronizacija ne usporava izvrÅ¡avanje.

---

## Direktiva barijere

- Direktiva barijere (`#pragma omp barrier`) sluÅ¾i da natera niti da se sinhronizuju. Gde god da se stavi u kodu definiÅ¡e graniÄnu taÄku.
- Kada bilo koja nit stigne to graniÄne taÄke pauzira dok sve druge niti nisu, takoÄ‘e, stigle do graniÄne taÄke. Tek onda se izvrÅ¡avanje nastavlja.

---

## Direktiva jednostrukog izvrÅ¡avanja

- Direktiva jednostrukog izvrÅ¡avanja (`#pragma omp single`) definiÅ¡e blok koda koji se izvrÅ¡ava u samo jednoj niti.
- Radi kao direktiva glavne niti osim Å¡to:
    - VaÅ¾i za prvu nit koja stigne do nje.
    - Zahteva sinhronizaciju na kraju bloka, tj. druge niti Ä‡e Äekati dok se ne zavrÅ¡i izvrÅ¡avanje bloka pod direktivnom jednostrukog izvrÅ¡avanja.
- Ova druga razlika se moÅ¾e iskljuÄiti kroz upotrebu `nowait` dodatka iza single direktive (`#pragma omp single nowait`)

---
## Redukcija

- Neke operacije su lake za paralelizaciju poÅ¡to uzimaju n ulaza a proizvode n ili viÅ¡e izlaza koji su nezavisni.
- Onda ih je lako iscepati na delove.
- Recimo da hoÄ‡emo da izraÄunamo sinus svake vrednosti u nekom ogromnom nizu: vrednost elementa 400494 ne zavisi od vrednosti elementa 403222 i nije bitno da li ih raÄuna jedna nit ili viÅ¡e.
- Å ta kada imamo zavisnost?
- Pa, ponekad niÅ¡ta. Neki problemi jednostavno ne mogu da se paralelizuju ili zahtevaju lukavstvo (kako paralelizovati raÄunanje FibonaÄijevih brojeva?)
- Ali ponekad je zavisnost malo pravilnija i moguÄ‡e je koristiti za to specijalizovane konstrukte.

---

## `#pragma reduction`

- Postoji `reduction` pragma sa sintaksomn 
    - `#pragma reduction(op : var)`
- Tu je op operator koji moÅ¾e biti: `+, *, -, /, &, ^. ili |`
- var je promenljiva za rezultat
- Ima samo smilsa u paralelnom kontekstu
- Moa se odnositi na blok u kome se pojavljuje komad koda koji izgleda ovako: `var = var op` izraz

---

## Trivijalan primer

```c
#include <stdio.h>
#include <stdlib.h>
#include <opm.h>

int main () {
    const int N = 1024;
    int i;
    double a[N], suma=0;

    for (i=0; i<N; i++) a[i] = 1.0*i;
    
    #pragma omp parallel for default (shared) private(i) schedule(static, 4) reduction(+:suma)
    for (i= ;i<N;itt) {
        result[i] += a[i];
    }

    printf ("Suma niza je %f\n", result[19]);
    
    return 0;
}

```

.footer.medium[
[Detaljnije](https://www3.nd.edu/~zxu2/acms60212-40212/Lec-12-OpenMP.pdf)
]

---

## FibonaÄijevi brojevi?

- FibonaÄijevi brojevi su divan primer situacije gde tradicionalne tehnike paralelizacije nisu osobito korisne.
- Konvencionalni algoritam za njih je fundamentalno serijski.
- Å ta znaÄi fundamentalno ovde?
    - `N-ti` korak zavisi od `n-1` i `n-2` koraka.
    - `N-1` korak zavisi od `n-2` i `n-3` koraka
    - `N-2` korak zavisi od `n-3` i `n-4` koraka
    - itd.
- Kako onda?
    - Varanje!

---

## Fina umetnost varanja

- ReformuliÅ¡emo problem tako da odgovara naÅ¡im potrebama.
- Postoji rekurzivna implementacija raÄunanja fibonaÄijevog broja, ali takoÄ‘e postoji direktna formula (izvinjavam se za uÅ¾asavajuÄ‡u Office tipografiju.)

<p>

$$ F_{n}=\sum_{k=0}^{|\frac{n-1}{2}|}\left(\begin{array}{c}n-k-1\\ k\end{array}\right) $$
  
</p>

---

## Fina umetnost varanja

- Da li ovo pomaÅ¾e? 
    - Ne skroz. 
- RaÄunanje binomijalnih koeficijenata je bazirano na raÄunanju faktorijela Å¡to opet stvara umeren problem. MoÅ¾e se paralelizovati (to je proizvod niza umesto sume), ali zahteva ugnjeÅ¾davanje paralelizama Å¡to moÅ¾e da bude nepotpuno podrÅ¾ano na naÅ¡oj arhitekturi.
- MoÅ¾emo da budemo direktniji, moÅ¾da, naroÄito ako hoÄ‡emo da raÄunamo fibonaÄijeve brojeve zaredom.

---

## Fina umetnost varanja

<p>

$$ F(n+2)= F(n+1)+F(n) \\
F(n+3)= F(n+1)+F(n+2)=F(n+1)â‹…2+F(n) \\
F(n+4)= F(n+2)+F(n+3)=F(n+1)â‹…3+F(n)â‹…3 \\
F(n+5)= F(n+3)+F(n+4)=F(n+1)â‹…5+F(n)â‹…4 \\
F(n+6)= F(n+4)+F(n+5)=F(n+1)â‹…8+F(n)â‹…5 \\
\quad \\
F(n + k)= F (n + 1) â‹… F(k) + F(n) â‹… F(k âˆ’ 1) $$
  
</p>

---

## Fina umetnost varanja

- Sada je naÅ¡ zadatak jednostavan: izraÄunamo m FibonaÄijevih brojeva i Äuvamo ih u memoriji
- Zatim sve brojevo od `m+1` do `2m` raÄunamo u m-tostrukoj paraleli uzimajuÄ‡i `n = m` a `k = 1..m`.
- Onda samo pomerimo da je `n = 2m+1` i ponovimo proces
- Drugim reÄima, naÅ¡ algoritam je serijski korak prekomputacije, a zatim serijsko ponavljanje m-tostruko paralelizovanog raÄunanja bloka vrednosti
- Stepen paralelizacije je, onda, praktiÄno neograniÄen.
- Pobeda!
- ViÅ¡e o ovakvim egzibicijama za Äas-dva.

---
name: openmpi
class: center, middle, inverse
layout: false

# OpenMPI
#### tehnologija prosleÄ‘ivanja poruka

---
layout: true

.section[[OpenMPI](#sadrzaj)]

---

## Ako imamo OpenMPâ€¦

- â€¦Äemu ovo?
- OpenMPI je optimizovan za situacije gde je model deljene memorije jednostavno nije primenjljiv.
- MPI je skraÄ‡enica od *Message Passing Interface* i predstavlja metod kojim se reÅ¡ava programiranje masivno paralelnih arhitektura.
- U jednom trenutku udarimo u limit na broj procesora koji moÅ¾emo nagurati u jedno kuÄ‡iÅ¡te: ako niÅ¡ta drugo, svi ti procesori se bore oko iste memorije, a transfer stope memorije su priliÄno oÅ¡tro ograniÄene.
- Postoje situacije gde ta ograniÄenja ne vaÅ¾e (i to Ä‡e biti vrlo detaljno istraÅ¾eno kada budemo priÄali o OpenACC-u)ali u opÅ¡tem sluÄaju podeljena memorija i vrlo paÅ¾ljiva komunikacija su naÅ¡a jedina opcija.

---

## Istorija i poreklo

- MPI nije biblioteka nego specifikacija.
- Kolekcija standarda koja specificira naÄin programiranja i protokol mreÅ¾ne komunikacije.
- Entoni Hoar je principe `message passing` modela postavio 70-tih godina proÅ¡log veka, a 1992 je tim predstavnika akademije i industrije je postavio MPI specifikaciju voÄ‘en struÄnjacima kao Å¡to je Viljem D. Grop.
- MPI ima verzije (MPI-1, MPI-2, MPI-3 itd.) koje mogu biti implementirane razliÄitim skupovima alata i biblioteka.
- MPICH je referentna implementacija.
- OpenMPI je opÅ¡ta implementacija.
- Mnogo specijalizovanih implementacija postoji.

---

## Struktura

- Kao i OpenMP, OpenMPI nije jezik.
- Odavno je primeÄ‡eno da posebni jezici retko doÅ¾ivljavaju Å¡iroku prihvaÄ‡enost.
- OpenMPI je:
    - Protokol (OSI nivo 5)
    - Biblioteka koja proÅ¡iruje postojeÄ‡e jezike sa konstruktima za paralelizam
    - Alati

---

## Osnovna implementacija

.lcol[

```c
#include <stdio.h>
#include "mpi.h"

int main(int argc, char *argv[]) {
    int size, rank;

    MPI_Init(&argc, &argv);
    MPI_Comm_size(MPI_COMM_WORLD, &size);
    MPI_Comm_rank(MPI_COMM_WORLD, &rank);

    printf("Hello World iz %d/%d.\n", rank, size);

    MPI_Finalize();

    return 0;
}
```

]

.rcol[

- `mpi.h` je obavezno zaglavlje
- Komanda `MPI_Init` uvek poÄinje izvrÅ¡avanje
- `MPI_Finalize` mora biti zadnja mpi komanda

]

---

## Osnovna implementacija

```console
mt@mt:~/MPI/solutions$ mpicc mpi_hello_world.c -o mpi_hello_world
mt@mt:~/MPI/solutions$ mpirun -np 4 ./mpi_hello_world
Hello world from 1
Hello world from 3
Hello world from 0
Hello world from 2

```

---

## Komunikacija

- Kao i veÄ‡ina ovakvih programa, ovo ne radi niÅ¡ta korisno.
- Da bi OpenMPI bio koristan, programi koji se paralelno izvrÅ¡avaju moraju da komuniciraju.
    - Ovo i nije baÅ¡ 100% taÄno. MoguÄ‡e je drÅ¾ati programe potpuno nezavisno u tkzv. `share nothing` modelu koji reÅ¡ava priliÄno Å¡irok dijapazon problema poznat kao `throughtput`.
    - Rendering je dobar primer.
    - Ali za ovo bi koristili SLURM i niÅ¡ta drugo. OpenMPI je Äist viÅ¡ak.
- Ne mogu komunicirati na naÄin na koji to radi OpenMP, preko deljene memorije, zato Å¡to iako u naÅ¡em primeru malopre sve Äetiri instance su na istom raÄunaru (i istoj ps tabeli) OpenMPI je namenjen da radi na potencijalno udaljenim raÄunarima.

---

## Komunikator

- Komunikator u OpenMPI-u je kolekcija procesa tj. nezavisnih pokrenutih instanci naÅ¡eg programa i moÅ¾e se porediti sa radio frekvencijom ili kanalom na `IRC/Discord serveru`.
- Proces moÅ¾e uÄestvovati u proizvoljnom broju komunikatora: oni su tu da bi se komunikacija lakÅ¡e organizovala.
- Minimalan broj komunikatora je jedan: svaki MPI program apsolutno mora imati barem jedan komunikator koji ne moramo da stvaramo: `MPI_COMM_WORLD`.
- `MPI_COMM_WORLD` je globalni kanal komunikacije: svi procesi su deo njega.

---

## API komunikatora

- `MPI_Comm` je tip identifikatora komunikatora
- Postoje komande koje sluÅ¾e za manipulaciju komunikatorom koje sve primaju kao parametar identifikator komunikatora

---

## VeliÄina komunikatora

.lcol[
- VeliÄina komunikatora je broj procesa pridruÅ¾enih komunikatoru.
- MPI komande tipiÄno vraÄ‡aju vrednost preko parametara, ne povratne vrednosti.
]

.rcol[

```c
MPI_Init(&argc, &argv);
MPI_Comm_size(MPI_COMM_WORLD, &size);
MPI_Comm_rank(MPI_COMM_WORLD, &rank);

printf("Hello World iz %d/%d.\n", rank, size);

MPI_Finalize();

return 0;
```

]

---

## Red procesa

- Red (rank) procesa je njegov identifikator unutar komunikatora.
- U pitanju je ceo broj nasumiÄno dodeljen u okviru komunikatora u opsegu 0..size-1

```c
int main(int argc, char *argv[]) {
    int size, rank;

    MPI_Init(&argc, &argv);
    MPI_Comm_size(MPI_COMM_WORLD, &size);
    MPI_Comm_rank(MPI_COMM_WORLD, &rank);

    printf("Hello World iz %d/%d.\n", rank, size);

    MPI_Finalize();

    return 0;
}
```

---

## Nedeterminizam izvrÅ¡avanja

```console
mt@mt:~/MPI/solutions$ mpirun -np 4 ./mpi_hello_world
Hello world from 1
Hello world from 3
Hello world from 0
Hello world from 2
mt@mt:~/MPI/solutions$ mpirun -np 4 ./mpi_hello_world
Hello world from 1
Hello world from 2
Hello world from 3
Hello world from 0
mt@mt:~/MPI/solutions$ mpirun -np 4 ./mpi_hello_world
Hello world from 0
Hello world from 1
Hello world from 3
Hello world from 2
mt@mt:~/MPI/solutions$ mpirun -np 4 ./mpi_hello_world
Hello world from 0
Hello world from 1
Hello world from 2
Hello world from 3

```

---

## Slanje poruka na specifiÄnu adresu

```c
MPI_Send(void *message, int count, MPI_Datatype datatype, int dest, int tag, MPI_Comm comm)
```

- `message` â€” pokazivaÄ na poruku generiÄkog tipa
- `count` â€” koliko elemenata ima u poruci
- `datatype` â€” Kog je tipa poruka
- `dest` â€” red procesa kome se Å¡alje
- `tag` â€” celobrojna vrednost rezervisana za proizvoljnu upotrebu
- `comm` â€” komunikator koji se koristi

---

## Primanje poruka

```c
MPI_Recv(void *message, int count, MPI_Datatype datatype, int source, int tag, MPI_Comm comm, MPI_Status *status)
```

- `message` â€” pokazivaÄ na poruku generiÄkog tipa gde smeÅ¡tamo vrednost koju dobijemo
- `count` â€” koliko elemenata ima u poruci
- `datatype` â€” Kog je tipa poruka
- `source` â€” red procesa od koga se prima
- `tag` â€” celobrojna vrednost rezervisana za proizvoljnu upotrebu
- `comm` â€” komunikator koji se koristi
- `status` â€” podaci o poruci ukljuÄujuÄ‡i ko je stvarno Å¡alje i Å¡ta je poslat tag

---

## Osnovni MPI tipovi MPI tip

.center-table.small[

|     **MPI tip**    |      **C tip**     |
|:------------------:|:------------------:|
|      MPI_CHAR      |     signed char    |
|      MPI_SHORT     |  signed short int  |
|       MPI_INT      |     signed int     |
|      MPI_LONG      |   signed long int  |
|  MPI_UNSIGNED_CHAR |    unsigned char   |
| MPI_UNSIGNED_SHORT | unsigned short int |
|    MPI_UNSIGNED    |    unsigned int    |
|  MPI_UNSIGNED_LONG |  unsigned long int |
|      MPI_FLOAT     |        float       |
|     MPI_DOUBLE     |       double       |
|   MPI_LONG_DOUBLE  |     long double    |
|      MPI_BYTE      |    unsigned char   |

]

---

## KorisniÄki tipovi podataka

- MoguÄ‡e je dodati i naÅ¡ tip podataka baziran na `struct`-u
- NaÅ¾alost sintaksa je malo kompleksna
- Tip se prvo definiÅ¡e a zatim upiÅ¡e

---

## Definisanje tipa

```c
MPI_Type_create_struct(
    int number_items,
    const int *blocklength,
    const MPI_Aint *array_of_offsets,
    const MPI_Datatype *array_of_types,
    MPI_Datatype *new_datatype)

```

---

## Upisivanje tipa

```c
MPI_Type_commit(MPI_Datatype *new_datatype)
```

---

## Primer

.small[

```c
#include <assert.h>
#include <stdio.h>
#include <string.h>
#include <mpi.h>

int main(int argc, char **argv){
    char buf[256];
    int my_rank, num_procs;
    MPI_Init(&argc, &argv); /* Initialize the infrastructure necessary for communication */
    MPI_Comm_rank(MPI_COMM_WORLD, &my_rank);     /* Identify this process */
    MPI_Comm_size(MPI_COMM_WORLD, &num_procs);  /* Find out how many total processes are active */
    /* Until this point, all programs have been doing exactly the same. Here, we check the rank to distinguish the roles of the programs */
    if (my_rank == 0) {
        int other_rank;
        printf("We have %i processes.\n", num_procs);
        for (other_rank = 1; other_rank < num_procs; other_rank++) /* Send messages to all other processes */
        {
            sprintf(buf, "Hello %i!", other_rank);
            MPI_Send(buf, sizeof(buf), MPI_CHAR, other_rank, 0, MPI_COMM_WORLD);
        }
        for (other_rank = 1; other_rank < num_procs; other_rank++) /* Receive messages from all other process */
        {
            MPI_Recv(buf, sizeof(buf), MPI_CHAR, other_rank, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);
            printf("%s\n", buf);
        }
    } else {
        MPI_Recv(buf, sizeof(buf), MPI_CHAR, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE); /* Receive message from process #0 */
        assert(memcmp(buf, "Hello ", 6) == 0),
        sprintf(buf, "Process %i reporting for duty.", my_rank); /* Send message to process #0 */
        MPI_Send(buf, sizeof(buf), MPI_CHAR, 0, 0, MPI_COMM_WORLD);
    }
    MPI_Finalize(); /* Tear down the communication infrastructure */
    return 0;
}
```
]

---

## Primer

```console
mt@mt:~/MPI$ mpicc mpi_send_rec.c && mpiexec -n 4 ./a.out
We have 4 processes.
Process 1 reporting for duty.
Process 2 reporting for duty.
Process 3 reporting for duty.

```

---

## Primer

- Ovo je klasiÄna implementacija radnik-menadÅ¾er obrasca u MPI.
- Nema niÄeg posebnog oko procesa sa redom 0, ali ga je lako naÄ‡i.
- Primetite da je izlaz ovog procesa sada deterministiÄki.

---

## Barijerna sinhronizacija

- Neke komande nisu namenjene posebnom procesu: neke sluÅ¾e za adresiranje viÅ¡e procesa
- To su kolektivne komande (`collectives`)
- Primer toga je `MPI_Barrier`(`MPI_Comm communicator`)
- Ovo je zaustavljanje izvrÅ¡avanja dok svi procesi u komunikatoru nisu stigli na isto mesto.
- Ovo je 1:1 ekvivalentno sa `#pragma omp barrier`

---

## Kolektivna komunikacija

- Kolektivna komunikacija opisuje situaciju gde imamo viÅ¡e procesa koji komuniciraju sa viÅ¡e procesa.
- MoÅ¾e se isprogramirati Å¡tagod da je to Å¡to Å¾elim, ali ovaj tip komunikacije pada uglavnom u Äetiri obrasca:
    - emitovanje (broadcast)
    - rasipanje (scatter)
    - skupljanje (gather)
    - sinhronizovano skupljanje (allgather)

---

## Broadcast

- Broadcast sluÅ¾i da podatak sa jednog procesa premesti na sve ostale ciljane procese.
- MoÅ¾e da koristi da osveÅ¾i nekakvu tabelu meÄ‘urezultata, ali je najkorisnije prilikom poÄetka rada procesa.
- Divan primer broadcast-a se moÅ¾e videti ako razmiÅ¡ljamo o onoj fibonaÄi implementaciji i njenom prvom koraku

---

## Broadcast

```c
int MPI_Bcast(void* shared_data, int number, MPI_Datatype datatype, int source_process, MPI_Comm communicator)
```

- Svi procesi pozivaju istu funkciju. Razlika je u tome Å¡to onaj proces Äiji je red jednak `source_process` parametru Äita iz `shared_data`, a ostali samo smeÅ¡taju ono Å¡to dobiju tu.

---

## Scatter

- Scatter, kao i broadcast, jeste sluÄaj slanja sa jednog na viÅ¡e procesa
- Razlika je u tome Å¡to proces koji Å¡alje takoÄ‘e podeli ono Å¡to Å¡alje u onoliko (tipiÄno ne-preklapajuÄ‡ih) podskupa koliko ima procesa kojima se Å¡alje.
- Ako se razmisli, u onom fibonaÄi primeru posle broadcast-a treba scatter koji podeli skup rednih brojeva fibonaÄijevog broja koji se raÄuna izmeÄ‘u procesa

---

## Scatter

```c
MPI_Scatter(void *send_data, int send_number, MPI_Datatype datatype, void *put_data, int put_number, int source_rank, MPI_Comm communicator)
```

- `send_data/number` je bafer za slanje (prazan i nebitan za sve osim za proces koji ima `source_rank` red)
- `put_data/number` je bafer za primanje koji svi koriste.

---

## Gather

- Gather je obrnut proces od scatter (oÄigledno).
- ViÅ¡e procesa Å¡alje jednom procesu delove nekog veÄ‡eg skupa podataka koji se zatim spaja.
- Posle broadcast i scatter faze, naÅ¡ fibonaÄi algoritam bi imao gather da iz procesa izvadi podatke o sledeÄ‡ih m brojeva i skupi ih u niti koja upravlja podacima.

---

## Gather

```c
MPI_Gather(void *send_data, int send_number, MPI_Datatype datatype, void *put_data, int put_number, int destination_rank, MPI_Comm communicator) 
```

- `send_data/number` je bafer za slanje (koji svi koriste) `put_data/number` je bafer za primanje (koji je prazan i nebitan za sve osim procesa sa redom koji je ravan `destination_rank`)

---

## Allgather

- Allgather je isto Å¡to i gather samo Å¡to ga odmah prati broadcast noga Å¡to se gather-uje.
- FibonaÄi program bi, u stvari, imao allgather operaciju umesto gather operacije za sve blokove osim poslednjeg.

---

## Allgather

```c
MPI_Allgather(void *send_data, int send_number, MPI_Datatype datatype, void *put_data, int put_number, MPI_Comm communicator)
```

- `send_data/number` je bafer za slanje (koji svi koriste) `put_data/number` je bafer za primanje (koji svi koriste)
- Naravno, mogli bi i da radimo gather praÄ‡en sa `broadcast`-om, ali nema razloga.

---

## Redukcije

- Redukcije su olakÅ¡ica koja omoguÄ‡ava kompleksnu komunikaciju neophodnu da se paralelizuje, npr. sumiranje niza i sliÄne operacije.
- Apsolutno je ista namena kao ekvivalentne OpenMP konstrukcije.
- Veoma je sliÄna sintaksi gather komande:

```c
int MPI_reduce)const void *send_data, void *put_data, int send_number, MPI_Datatype, MPI_OP operation, int destination_rank, MPI_Comm comm)
```

- Jedina razlika jeste operacija koja sluÅ¾i za kombinovanje koja moÅ¾e biti korisniÄki definisana ili jedna od osnovnih operacija

---

## Redukcioni operatori

.center-table.small[

|          **Operacija**         | **Operator** |
|:------------------------------:|:------------:|
|            Maksimum            |    MPI_MAX   |
|             Minimum            |    MPI_MIN   |
|              Suma              |    MPI_SUM   |
|            Proizvod            |   MPI_PROD   |
|            LogiÄko I           |   MPI_LAND   |
|              Bit I             |   MPI_BAND   |
|           LogiÄko ILI          |    MPI_LOR   |
|             Bit ILI            |    MPI_BOR   |
|           LogiÄko XOR          |   MPI_LXOR   |
|             Bit XOR            |   MPI_BXOR   |
| Maksimalna vrednost i lokacija |  MPI_MAXLOC  |
|  Minimalna vrednost i lokacija |  MPI_MINLOC  |

]

---

## Sinhronizovana redukcija

- Kao Å¡to je Reduce bazirano na Gather, Allgather proizvodi Allreduce
```c
int MPI_Allreduce(const void *send_data, void *put_data, int send_number, MPI_Datatype datatype, MPI_OP operation, MPI_Comm comm)
```

---

## Svi/svi komunikaciona Å¡ema

- To je Å¡ema komunikacije koja:
- AranÅ¾ira sve procese tako da formiraju matricu gde su
    - Redovi procesi
    - Kolone particije podataka kao Å¡to bi ih scatter komanda napravila
- Formira transfer podataka iz izlaznog u ulazni bafer tako da efektno transponira podatke

---

## Svi/svi komunikaciona Å¡ema

.lcol[

![:scale 65%](img/l.png) 
]

.rcol[

![:scale 65%](img/r.png)

]

---

## Svi/svi komunikaciona Å¡ema

```c
int MPI_Alltoall(void *send_data, int send_number, MPI_Datatype send_datatype, void *put_data, int put_number, MPI_Datatype put_datatype, MPI_Comm communicator)
```

---

## NeblokirajuÄ‡a usmerena komunikacija

- Do sada, svo slanje podataka je blokirajuÄ‡e
- Ako nema Recv za svako Send program stane.
- TakoÄ‘e, imamo nuÅ¾no sinhrono ponaÅ¡anje, to moÅ¾e da uspori program: ako ne moramo da sinhronizujemo, ne treba.
- Radi isto kao ranije, ali vraÄ‡a MPI_Request objekat

```c
int MPI_Isend(void *message, int count, MPI_Datatype datatype, int dest, int tag, MPI_Comm comm, MPI_Request *send_request)
int MPI_Irecv(void *message, int count, MPI_Datatype datatype, int source, int tag, MPI_Comm comm, MPI_Request*receive_request)
```

---

## MPI_Request

- Ovo je promenljiva koja pokazuje na potencijalno neispunjenu operaciju slanja/primanja
- Kada je imamo, moÅ¾emo je koristiti da saÄekamo da se operacija zavrÅ¡i, sinhronizujuÄ‡i naÅ¡ poziv kada zaÅ¾elimo: <br>`int MPI_Wait(MPI_Request *req, MPI_Status *status)` 
- Ovo Ä‡e vratiti status kada se operacija bude zavrÅ¡ila.
- MoguÄ‡e je i asinhrono proveriti da li se operacija zavrÅ¡ila kroz: <br>`int MPI_Test(MPI_Request *req, int *flag, MPI_Status *status)`
- Ovo se odmah zavrÅ¡i i postavi flag na true ako je operacija gotova i status na vrednost statusa ako je flag true.

---

name: kaaa
class: center, middle, inverse
layout: false

# Koncept akceleratora i akceleratorske arhitekture

#### finalni gradivni element heterogenih super-raÄunarskih arhitektura

---
layout: true

.section[[Koncept akceleratora i akceleratorske arhitekture](#sadrzaj)]

---

## Å ta je akcelerator?

- Procesor raÄunara je napravljen da, manje-viÅ¡e, bude beskonaÄno podesiv i prigodan bilo kom poslu.
- Iako su odreÄ‘ene primene brÅ¾e sve je u principu moguÄ‡e na CPU arhitekturi.
- Ovo se plaÄ‡a: univerzalnost procesora znaÄi da on nije maksimalno prilagoÄ‘en nijednom poslu.
- Akcelerator je komponenta koja je deo raÄunara pored procesora, a koja omoguÄ‡ava raÄunaru da radi neke stvari brÅ¾e kroz hardver optimizovan za specifiÄan scenario koriÅ¡Ä‡enja.

---

## Tipovi akceleratora

- Akcelerator se najbolje razlikuje po svojoj poziciji u sistemu.
- Akcelerator moÅ¾e biti:
    - Na Äipu
    - Koprocesori
    - Na magistrali
- Akceleratori na Äipu su deo samog procesora i predstavljaju specijalizovanu komponentu u njima.
- Koprocesori su montirani odmah uz procesor, ali u posebnom pakovanju.
- Akceleratori na magistrali su odvojene komponente koje su nekakvom magistralom povezane za nekakav centralni procesor ili procesore.

---

## Tipovi akceleratora

- Akceleratori koji su na Äipu komuniciraju sa procesorom izuzetno brzo iz oÄiglednih razloga.
- Problem instaliranja akceleratora na Äip jeste u tome Å¡to:
    - To poveÄ‡ava toplotno/elektriÄne zahteve prema Äipu koji se koristi Å¡to nije beskonaÄan resurs.
    - PoveÄ‡ava kompleksnost Äipa Å¡to se direktno negativno odraÅ¾ava na cenu.
    - NuÅ¾no deli istu vezu sa memorijom koju normalno koristi procesor Å¡to je u skoro svakom sistemu ozbiljno usko grlo.
    - Akceleratori se i dalje integriÅ¡u na Äipove, ali samo u posebnim prilikama.

---

## Tipovi akceleratora

- Akcelerator na magistrali ima odreÄ‘ene izuzetno znaÄajne prednosti:
    - Problem upravljanja temperaturom i strujom je odvojen od glavnog procesora te ga je lakÅ¡e reÅ¡iti.
    - Akcelerator je u svom, jeftinijem, Äipu.
    - Akcelerator moÅ¾e da ima svoje memorijske resurse.
- Glavan mana akceleratora u ovoj formi jeste Å¡to stvara novo, kljuÄno, usko grlo, a to je magistrala koja ga povezuje sa procesorom koji, nuÅ¾no, mora funkcionisati kao kontroler i obavljati poslove uÄitavanja podataka iz glavne memorije i smeÅ¡tanja rezultata u glavnu memoriju.
- Ovo ograniÄenje takoÄ‘e smanjuje sposobnost upravljanja akceleratorskim resursima buduÄ‡i da su oni izolovani u svom malom svetu.
- Ova ograniÄenja se reÅ¡avaju programski.

---

## Istorija akceleratora â€” koprocesori

- Koprocesori su, fundamentalno, dodatni Äipovi koji proÅ¡iruju set instrukcija raÄunara kroz hardverski-implementirane brze verzije operacija iz nekog specifiÄnog domena.
- Koprocesori mogu imati svoju memoriju (obiÄno jako malo) ili deliti glavnu ili i jedno i drugo.
- U liÄnim raÄunarima i igraÄkim konzolama, koprocesori su dugo bili glavni naÄin na koji su se relativno jeftino proizvodili impresivni efekti u performansama.
- Cena ovakve arhitekture na nivou individualne maÅ¡ine jeste u neuniformnosti programiranja i tome Å¡to je fleksibilnost maÅ¡ine dramatiÄno ograniÄena.
- Ovo je fantastiÄno ilustrovano u istoriji raÄunarskih igara u perioduranih 90-tih.

---

## Koprocesori

- Koprocesori mogu da imaju izuzetno Å¡irok dijapazon primena, recimo:
    - I/O kroz pametan DMA transfer
    - video/audio kodiranje/dekodiranje i demultipleksiranje
    - kriptografija
    - DSP
    - *grafika*

---

## Sudbina koprocesora

- Koprocesori su i dalje sa nama, ali su uglavnom migrirali, specifiÄno:
    - Dosta I/O funkcionalnosti je (buduÄ‡i da je lako) zavrÅ¡ilo u 'omnibus' Äipovima na matiÄnim ploÄama koji viÅ¡e nego adekvatno obavljaju sve ove poslove.
    - OdreÄ‘ene funkcije su ugraÄ‘ene direktno u procesor ukljuÄujuÄ‡i FPU, MMX, i sliÄno.
    - Ostatak funkcionalnosti je zavrÅ¡io u posebnim akceleratorskim dodacima, ponajviÅ¡e u grafiÄkim karticama.

---

## Istorijski primer koprocesoraâ€” Intel 8087

![:scale 75%](img/kop.png)

---

## Operacije sa pokretnim zarezom

- Operacije sa pokretnim zarezom nisu lake
- Sabrati dva cela broja je lako na raÄunaru
- Svodi se na XOR uz upotrebu bita prenosa, lako je kolo koje to radi napraviti (uistinu, ne moramo, TTL IC 7483 je baÅ¡ to)
- Sabiranje dva broja sa pokretnim zarezom?

---

## Sabiranje/oduzimanje brojeva sa pokretnim zarezom

![:scale 40%](img/sab.png)

---

## Relativna kompleksnost FLOPS-ova

- Kao rezultat ovoga za liÄne raÄunare, naroÄito, operacije sa pokretnim zarezom su bile gotovo nemoguÄ‡e spore.
- To je znaÄilo da Äitave kategorije proraÄuna ne mogu da se efektno rade.
- Ali, buduÄ‡i da su algoritmi za operacije na brojevima pokretnog zareza poznati i definisani `IEEE 754` standardom, moguÄ‡e je implementirati ih u hardveru i ubrzati operacije pokretnim zarezom dramatiÄno: to je Intel i uradio Äipom `Intel 8087`

---

## Intel 8087

- Trik je bio da se u maÅ¡inskom kodu ubaci `ESCAPE` komanda koja bi predavala kontrolu koprocesoru.
- Procesor bi (ne znajuÄ‡i da koprocesor uopÅ¡te postoji) izvrÅ¡io oÄitavanje memorije koju instrukcija pominje (ako je ima), a onda bi te podatke presretnuo `8087` Äip.
- Teoretski govoreÄ‡i, procesor i koprocesor su mogli da istovremeno izvrÅ¡avaju instrukcije.
- Teoretski.
- U praksi bi se pobili oko magistrale i sruÅ¡ili maÅ¡inu do taÄke da je bio potreban hardverski restart. Zbog toga veÄ‡ina kompajlera iz tog perioda ubacuje `WAIT` komande odmah posle FPU komandi.
- Ovo je divna ilustracija problema `on-chip` i koprocesorski akceleratora.

---

## Evolucija Intel 8087

- `8087` je unapreÄ‘ivan onako kako su unapreÄ‘ivani i procesori: `80187`, `80287`, `80387`.
- `80387` koji je uparen sa Intel 80386 procesorom (koji je na vrlo realan naÄin direktan predak modernih procesora, `AMD i Intel`)je prvi `Intel FPU` koji je u potpunosti kompatibilan sa `IEEE 754` standardom (revizija iz 1985).
- `80486` serija procesora (i sve od tada do danas) integriÅ¡e `FPU` na samom Äipu.

---

## MatematiÄki koprocesori kao akceleratori na magistrali

- U ranim danima liÄnih raÄunara, postojala je ogromna glad za dobrim performansama u proraÄunima sa pokretnim zarezom.
- Dosta ljudi je htelo raÄunara da raÄuna neÅ¡to, a veÄ‡ina proraÄuna koju ljudi Å¾ele ukljuÄuje decimalne zareze.
- Ovo je stvorilo znaÄajno trÅ¾iÅ¡te za posebne ureÄ‘aje za ubrzane proraÄune.
- Primer toga je `Weitek` familija izuzetno (za to vreme) brzih `FP akcelerator`a (`Intel 8087` ostvaruje `~50 kFLOPS`-a, `Weitek WTL 3167` je mogao da ostvari `5.6 MFLOPS`-a samo par godina kasnije).
- `Weitek` ureÄ‘aji nisu bili integrisani u procesor, no su bili dodatni ureÄ‘aji.

---

## Rani dani akceleratora na magistrali

- Lako je staviti akcelerator na magistralu: kako on radi?
- Rani akceleratori ove vrste su sami implementirali svoj `I/O` mapiran na memoriju.
- To znaÄi da su se `Weitek` ureÄ‘aji pretvarali da su dodatan modul memorije i procesor je sa njima komunicirao tako Å¡to je izvrÅ¡avao potpuno obiÄne komande za premeÅ¡tanje memorijskog sadrÅ¾aja.
- PC arhitektura i dans koristi memorijsko mapiranje ove vrste, ali retko tako `divlje` kao Å¡to su rani Weitek ureÄ‘aji koristili.
- Kasniji modeli su i emulirali 8087 seriju (kroz presretaÄ na utiÄnici za koprocesor) i komunicirali na standardniji naÄin kao Å¡to je, na primer, `EISA VESA local bus` tehnologija.

---

## Moderni magistralni akceleratori pre GPU revolucije

- Pre nego Å¡to je GPU tehnologija zavladala kao dominantna u svetu akceleratora, postojale su posebne kartice koje su radile istu stvar specifiÄno za HPC svrhe.
- Recimo, `ClearSpeed Advance X620`
    - Dva procesora
    - PCI-X interfejs sa DMA transferom
    - 50 GFLOPS
    - 1GB ECC memorije
    - 43W TDP

---

## ClearSpeed Advance X620

![:scale 65%](img/misc_20cpu.jpg)

---

## Arhitektura X620

- Svaki `X620` je imao dva `CSX600` procesora
- Svaki od tih procesora je imao jednu izvrÅ¡nu jedinicu i odreÄ‘eni broj (zavisi od revizije) procesnih jedinica koje su grupisane u dve sekcije:
    - `Mono` (obraÄ‘uje skalarne podatke kao blago glup procesor)
    - `Poly` (Niz od 96 procesnih elemenata koji obraÄ‘uje nizove podataka uz tkzv. enable registre kao mehanizam da se odreÄ‘eni elementi koriste ili ne.
- `Poly` sekcija je operisala u klasiÄnom `SIMD` reÅ¾imu.

---

## Programiranje sa X620

- Sam po sebi `X620` ne radi niÅ¡ta.
- Mora se ruÄno aktivirati.
- Da bi se to olakÅ¡alo `ClearSpeed` je napisao ekstenziju programskog jezika C (Cn) koja je dodala `poly` oznaku za memoriju koja je garantovala da Ä‡e ta vrednost biti pristuna (i sinhronizovana) izmeÄ‘u memorije procesora i memorije kartice.
- `ClearSpeed` je joÅ¡ morao i da distribuira posebne biblioteke koje su implementirale Äestu nauÄnu funkcionalnost, tj. `BLAS`, `FFT`, itd. na naÄin koji je mogao da koristi `ClearSpeed` proizvode.

---

## Mane akceleratora ove vrste

- Strahovito skupi
- MoÅ¾ete programirati samo u jeziku prozivoÄ‘aÄa!
- OgraniÄeni ste na biblioteke proizvoÄ‘aÄa ili one koje sami napiÅ¡te da koriste hardver kako treba.
- Konstantna briga oko lokacije podataka u memoriji
- `ClearSpeed` hardver je danas naÄisto zaboravljen
- ZaÅ¡to? 
    - GPU revolucija.

---

## GPU revolucija

- Super-raÄunari su bitni i skupi i sve je to jako lepo ali i najskuplji superraÄunar na svetu, strahovito impresivni `IBM Summit` je koÅ¡tao $325 000 000.
- To je dosta novca, ali drugi naÄin da se to kaÅ¾e jeste `onoliko novca koliko je zaradio Call of Duty Black Ops 4 za oko dva dana.`
- IgraÄka industrija je ogromna i kao rezultat:
    - MoÅ¾e da baci enormne sume novca na istraÅ¾ivanje
    - Ekonomski efekti skaliranja znaÄe da Ä‡e cena Äuda tehnologije koje proizvede to istraÅ¾ivanje biti dramatiÄno manja.
    - BuduÄ‡i da jako puno ljudi koristi i hoÄ‡e da koristi taj hardver softverska podrÅ¡ka Ä‡e biti mnogo univerzalnija.

---

## Å ta je GPU napravljen da radi?

- Prvi GPU-ovi (tada poznati pod imenom `video kartice`) su bili jako drugaÄije sprave nego danas i nisu imale nikakvu HPC primenu.
- Glavne funkcije GPU-ova te vrste su bile da digitalni signal (sadrÅ¾aj `framebuffer`-a) pretvori u signal koji se moÅ¾e prikazati na ekranu, u to doba gotovo uvek analogni signal.
- Dodatne funkcije GPU-ova ove vrste u ne PC arhitekturama su bile funkcije 2D ubrazanja: brz blt transfer, hardverski sprajtovi, hardverska paralaksaâ€¦
- Jako jako korisno ako pravite video igru, ali nije interesantno za nas
- GPU revolucija ima svoj koren u operacijama neophodnim za 3D grafiku.

---

## Å ta 3D grafika hoÄ‡e?

- NaÄin na koji se tradicionalna 3D grafika radi jeste da se:
- Napravi 3D reprezentacija onoga Å¡to treba da se vidi kao serija temena, i ivica meÄ‘u njih definiÅ¡uÄ‡i nekakvu geometriju.
- Ta 3D reprezentacija se anotira sa podacima koji opisuju detalje toga kako ta geometrija reaguje na svetlost:
    - Normale
    - Materijali
    - Mapiranje teksture
- Zatim se sistemu dodaju podaci o izvorima svetlosti, pozicije kamere i sliÄnim globalnim detaljima.
- Onda poÄne proraÄun koji poÄne od ovoga a zavrÅ¡i sa nizom vrednosti piksela, spremnim za (2D) prikaz.

---

## ProraÄun 3D grafike

- Svako teme se procesuira tako da je transformisano kako je to odgovarajuÄ‡e. Ovo se svodi na mnoÅ¾enje matrica i vektora.
- Temena se grupiÅ¡u u primitive u skladu sa specifikacijom.
- Geometrija scene se rasterizuje koristeÄ‡i raycast metod, rezultat ovoga su fragmenti. Fragmenti su preteÄe piksela.
- Fragmenti scene se osvetljavaju i teksturiraju.
- Fragmenti se uzorkuju u piksele.
- 2D pikseli se prikazuju.

---

## Osobine proraÄuna 3D grafike

- 3D grafika stalno vrÅ¡i kompleksne operacije pokretnog zareza nad velikim nizovima podataka.
- Drugim reÄima, grafiÄka kartica je odliÄan `SPMD akcelerator`.
- Ali, kako je opisana ovakva grafiÄka kartica je beskorisna za nas.

---

## Istorija ranih 3D akceleratorskih kartica

- Sve poÄinje od legendarnih SGI radnih stanica: `IRIX` i `OpenGL` i najranije forme 3D ubrzanja baziranog na `Quad` primitivu.
- Poseban hardver za arkadne maÅ¡ine.
- Na PC to stiÅ¾e kroz posebnu seriju `Voodoo` kartica kompanije `3Dfx`. Ovo su iskljuÄivo akceleratori 3D proraÄuna i zahtevaju posebnu 2D karticu.
- Kasnije `Voodoo Banshee, S3 ViRGE, ATI Rage i NVidia TNT` kartice integriÅ¡u 2D i 3D funkcionalnost.
- Ove kartice su, u poÄetku obavljale samo odreÄ‘ene deliÄ‡e 3D operacija, tipiÄno korekciju perspektive, mapiranje, i filtriranje. KarakteristiÄan 'mutan' izgled ove ere 3D ubrzanja proizvodi niska rezolucija tekstura i linearan reÅ¾im filtriranja

---

## Istorija ranih 3D akceleratorskih kartica

- Napredak je nastao sa `NVidia GeForce 256` (poznato ime?) i `ATI Radeon` (poznato ime?) kartciama (1999 i 2000) koje su uvele hardverske implementacije svih transformacija i osvetljenja.
- No, i sa tim dodacima ove kartice su za nas beskorisne.
- Å to?
- Zato Å¡to samo rade operacije nad grafikom.
- Imaju potencijal da brzo raÄunaiju, ali ne i naÄin da ih nateramo da to rade.
- Negde oko `GeForce` 6 generacije se polako uvodi koncept `programabilnih Å¡ejdera`

---

## Shader?

- Danas ta reÄ znaÄi neÅ¡to drugo (viÅ¡e o tome kasnije) ali u eri o kojoj govorimo Å¡ejder (*shader*) je bila operacija koja je punila delove ekrana nekakvim proraÄunatim vrednostima boja, tj. senÄila je deo ekrana.
- Rani shader-i su bili fiksni ali su mogli da se parametrizuju
- Kasnije, odreÄ‘ena koliÄina programiranja je bila, jedva, moguÄ‡a.
- To je bilo vrlo skuÄeno programiranje, doduÅ¡e, Äesto bez kontrole toka i ograniÄenim brojem funkcija i bez sposobnosti da se upravlja resursima kartice.

---

## Razvoj programabilnog shader-a

- VoÄ‘eni Å¾eljom programera video igara da mogu da implementiraju razliÄite grafiÄke algoritme sa veÄ‡om fleksibilnoÅ¡Ä‡u, moÄ‡ shader-a je napredovala, uvodeÄ‡i sve veÄ‡u kontrolu.
- To je eventualno dovelo do pada fiksnog toka izvrÅ¡avanja (*fixed function pipeline*) gde su opÅ¡ti stadijumi renderovanja bili hardverski uslovljeni i njegove zamene sa dinamiÄnim tokom izvrÅ¡avanja.
- Ovo je vrlo brzo posle toga proÅ¡ireno sa unificiranim shader-ima gde e oni sada tretiraju mnogo viÅ¡e kao programi koji se izvrÅ¡avaju nego parametrizacije fiksnih koraka.
- Ovaj razvoj dostiÅ¾e svoj vrhunac kroz moderne `Vulkan/DirectX 12` implementacije

---

## HPC primena

- U ovom stadijumu, primena GPU-ova za HPC je mnogo lakÅ¡a.
- GPU moÅ¾emo da opskrbimo teksturom i modelima koji su u stvari naÅ¡i podaci i onda `render` je u stvari naÅ¡ proraÄun.
- SreÄ‡om, ni ovo nije potrajalo, i moderni GPU-ovi imaju poseban reÅ¾im izvrÅ¡avanja namenjen proraÄunima.

---

## Arhitektura modernog GPU-a

- Brojke se odnose na Paskal seriju Nvidia kartica, ali same informacije bi trebale da vaÅ¾e u trenutku pisanja.
- VeÄ‡ina procesa je u samom Äipu kartice
- Centralni Äip (GPU) se sastoji od:
    - GrafiÄkih klastera (GPC-ova) (Paskal: 6 komada)
    - L2 keÅ¡a (Paskal: 4MB)
    - Kontrolera za memoriju (Paskal: 8 512-bitnih)
    - PCI Express kontrolera
    - GigaThread podsistem

---

## GPC

- Svaki `GPC` je potpun `mini-GPU` i radi sve Å¡to i `GPU`.
- `GPC` se deli u klastere teksturiranja koji se sastoje od glavne gradivne jedinice GPU-a: multiprocesore toka (*streaming multiprocessors, SM*)
- Svaki `SM` je procesorski element koji se sastoji od `CUDA` (*Compute Unified Device Architecture*) jezgara (Paskal: 64 jednostruke preciznosti i 32 dvostruke preciznosti) podeljenih u dva bloka od kojih svaki ima instrukcioni bafer, raspored niti, i 128KB registarske memorije. 
- `SM` takoÄ‘e ima 16 jedinica za rad sa memorijom i 16 jedinica za posebne funkcije aproksimacije.

---

## Memorija

- Ovde postoji oÅ¡tra razlika izmeÄ‘u potroÅ¡aÄkih modela i modela napravljenih baÅ¡ za `HPC`
- PotroÅ¡aÄki modeli i dalje koriste istu DDR memoriju kao i glavna maÅ¡ina, samo na visokoj brzini i povezanu kroz izuzetno Å¡iroku magistralu.
- Uprkos tome, pristup memoriji je znaÄajno usko grlo.
- `HPC` modeli koriste mnogo efikasniju `HBM2` memoriju
- `HBM2` koristi tehniku gde se GPU i kontroler memorije nalaze u istom fiziÄkom paketu na deljenom supstratu (silikonskom sistemu presretanja) koji omoguÄ‡ava vrlo brzu komunikaciju
- Sama memorija je u ploÄama na samom kontroleru, i povezana je ultramalim ultrabrzim vezama kroz silicijum samog kontrolera i drugih ploÄica memorije.

---

## HBV2

![:scale 75%](img/hbv2.png)

---

## Pristup memoriji

- Moderni GPU-ovi podrÅ¾avaju izuzetno kompleksan sistem pristupa memoriji koji koristi memorijske stranice da se postara da GPU kod moÅ¾e uniformno da pristupa svoj memoriji u raÄunaru.
- Ovo je veliko olakÅ¡anje, ali naravno, pristup glavnoj memoriji ima ogromnu cenu.
- TipiÄno, problem se reÅ¡ava tako Å¡to se memorija kopira u memorijski prostor kartice, no ovo stvara potencijalne probleme sa sinhronizacijom.
- Ovo je predmet intenzivnog istraÅ¾ivanja.

---

## Povezivanje

- Standardno povezivanje je preko `PCI Express` linka.
- `PCI Express` je brz (ali uvek moÅ¾e brÅ¾e) plus problem nastaje:
    - Å ta kada hoÄ‡u 4 kartice u jedan raÄunar?
- Svaka kartica hoÄ‡e 16 `PCI Express` linija za transport
- To je lepo, novi i9-9900K ima ukupno 16 `PCI Express` linija.
- Neki `Xeon` procesori imaju viÅ¡e, i `Threadripper` ima 32, ali, kao Å¡to se vidi, ukaÄiti dve kartice je teÅ¡ko
- ReÅ¡enje? 
    - `NVLink`
- `NVLink` je izuzetno visoko-performantni mehanizam za komunikaciju na male razdaljine.

---

## Povezivanje

- `NVLink` moÅ¾e da povezuje kartice (i tako bi tipiÄno i stavili dve kartice u proseÄan raÄunar), a i karticu i procesor ako to procesor podrÅ¾ava (IBM pravi procesore koji ovo mogu i oni su instrumentalni u funkcionisanju `Summit` maÅ¡ine).
- RazliÄite topologije su moguÄ‡e.
- `PCIExpress` ima specifiÄne svoje prednosti ako se koristi uz NVlink kroz `RDMA` funkcionalnost.
- `RDMA` omoguÄ‡ava da sa GPU-om komunicira memorija ili, joÅ¡ bolje, mreÅ¾na kartica apsolutno bez rada procesora. 
- Ovo dramatiÄno olakÅ¡ava dizajn masivno paralelnih hibridnih heterogenih arhitektura visokih performansi.

---

## Kako sve ovo programirati?

- `NVidia` ima `CUDA API`
    - Maksimum performansi
    - 0% portabilnosti
    - PriliÄno ruÅ¾na sintaksa koja zahteva poseban NVidia kompajler
    - OdliÄni alati
- `OpenCL`
    - Poseban jezik
    - Maksimalno univerzalan
- `OpenACC`
    - ProÅ¡irenje C-a preko pragmi u OpenMP maniru
    - Umereno univerzalan.
- I `OpenACC` i `OpenCL` Å¾rtvuju neÅ¡to performansi za svoju brzinu.

---

name: openacc
layout: false
class: center, middle, inverse

# OpenACC
#### uniformna tehnologija pristupa akceleratorskom hardveru

---
layout: true

.section[[OpenACC](#sadrzaj)]

---

## Koja je veza izmeÄ‘u `CUDA` i ovoga?

- `CUDA` je specifiÄna `Nvidia` tehnologija.
- `OpenACC` je otvorena specifikacija koja je namenjena da glatko podrÅ¾i Å¡irok dijapazon akceleratorskih ureÄ‘aja.
- Å ta karakteriÅ¡e baÅ¡ akceleratorski ureÄ‘aj?
    - Posebna namena
    - Posebna hijerarhija memorije
    - NeÄim ograniÄen transfer.
- `OpenACC` nam omoguÄ‡ava da piÅ¡emo optimizovan kod koji ovo uzima u obzir i sreÄ‘uje transfer mesto nas.
- To znaÄi da je promenljiva za nas jedan jedini objekat koji moÅ¾e biti u memoriji sistema i u memoriji akceleratorskog ureÄ‘aja.

---

## OpÅ¡ta struktura

- `OpenACC` deli dosta osobina u svom dizajnu sa `OpenMP` i `OpenMPI` tehnologijama.
- Sa oba deli to Å¡to pokuÅ¡ava da bude, u Å¡to je veÄ‡oj meri moguÄ‡e, standardni programski jezik (to jest, C)
- Sa `OpenMP` deli oslanjanje na pragme.
- Kao i `OpenMP`, podrÅ¡ka mora da bude ubaÄena direktnu u kompajler.
- `GCC 8` podrÅ¾ava `OpenACC 2.5, GCC 7 OpenACC 2.0a`

---

## NeÅ¡to termina

- DomaÄ‡in (eng. *host*)
    - DomaÄ‡in je raÄunar na kome se izvrÅ¡ava kod i koji ima neki broj akceleratora
- Prebacivanje (eng. *offloading*)
    - Prebacivanje je mehanizam kojim se posebno specificirani delovi koda prebacuju na akcelerator.

---

## Paralelizmi u OpenACC modelu

- `OpenACC` prepoznaje tri nivoa paralelizma kod akceleratora:
    - `Grub` (*coarse grain*)
    - `Fin` (*fine grain*), i
    - Individualan
- `Grub` paralelizam deli poslove na viÅ¡e resursa za proraÄun.
- `Fin` paralelizam tipiÄnu distribuira poslove na individualne procesne elemente.
- Individualan paralelizam je u okviru jednog `PE`, i izlaÅ¾e paralelizam `SIMD`/vektor tipa direktno.
- Ovi nivoi u `ACC`-u se predstavljaju gang, worker, i vector paralelizmima.

---

## Gang, Worker, Vector

- `Gang` (tim) je najviÅ¡i nivo paralelizma za `OpenACC` model.
- Svaki ureÄ‘aj akcelerator izvrÅ¡ava neki broj gang-ova koji imaju jedan ili viÅ¡e radnika (*worker*-a) koji izvrÅ¡avaju ili individualne ili vektorske instrukcije.
- Vektorski paralelizam mora biti vrlo uniforman: ista stvar za viÅ¡e podataka u klasiÄnom `SIMD` maniru.
- Worker paralelizam je malo opuÅ¡teniji ali deli iste resurse buduÄ‡i da svaki gang je, hardverski, definisan kao neÅ¡to Å¡to deli iste procesne resurse.
- U `NVidia` svetu to znaÄi da je `Gang` gotovo uvek vezan za jedan `SM`.
- `Gang` paralelizmi su nezavisni.

---

## Gang,Worker,Vector

![:scale 60%](img/gang.png)

---

## ReÅ¾imi izvrÅ¡avanja

- IzvrÅ¡avanje `OpenACC` koda uvek poÄinje u *Gang-Redundant* reÅ¾imu (`GR`). To je reÅ¾im u kome svaki gang ima jednog radnika koji izvrÅ¡ava isti kod. Drugim reÄima, nema paralelizma.
- Kada se u `OpenACC` kodu stigne to paralelnog segmenta prelazi se u *Gang-Partitioned* reÅ¾im (`GP`) u kome su iteracije jedne petlje (ili, Äak, viÅ¡e petlji) distribuirane izmeÄ‘u gang-ova, ali svaki gang i dalje ima samo jednog radnika koji radi na individualnim elementima u, tkzv. `Worker-Single` i `Vector-Single` reÅ¾imima.
- Na `GP nivou u WS/VS` reÅ¾imu mi smo veoma kao `OpenMP` i niti.
- Ako se to zatraÅ¾i, moguÄ‡e je aktivirati `Worker-Partitioned` reÅ¾im gde se posao deli izmeÄ‘u radnika, te je u okviru radnika moguÄ‡e zatraÅ¾iti  `Vector-Partitioned` reÅ¾im

---

## ReÅ¾imi izvrÅ¡avanja

- MoÅ¾e se o ovome misliti kao o `OpenMP`-u u kome je ugnjeÅ¾davanje paralelizma ne samo moguÄ‡e nego neophodno i to na tri nivoa.
- `OpenMP` niti su sve stvorene jednake, te nema performantnog razloga da preferiramo `3x2x2` niti u odnosu na `12 niti`.
- U `OpenACC` postoji hijerarhijska podela na hardverskom nivou, te ukupne niti se najbolje dele na `gang/worker/vector `po tome koliko su podaci nad kojima radi nit vezani jedni za druge.
- SpuÅ¡tati stvari na akcelerator nema puno smisla osim ako podaci nisu barem malo povezani i stoga prigodni. 
- SpuÅ¡tati stvari na vektorski nivo nema smisla osim ako (predvidivo) u pitanju nisu vektori.

---

## OpenACC Hello World

```c
#include <stdio.h>
#include <openacc.h>

int main(){

    printf ("Supported OpenACC revision> %d.\n", _OPENACC);

    int count = acc_get_num_devices(acc_device_nvidia);
    printf("Found %d NVidia GPUs.\n", count);

    int nm = acc_get_device_num(acc_device_nvidia) ;
    printf("default accelerator number is %d.\n", n);

    count = acc_get_num_devices(acc_device_host);
    printf (â€œFound %d host processor.\n", count);

    n = acc_get_device_num(acc_device_host);
    printf("Default host processor number is %d.\n", n);

    return 0;
}

// Kompajliranje OpenACC koda
mt@mt:~/OpenACC$ gcc acc.c -fopenacc -o acc
mt@mt:~/OpenACC$ ./acc 

```

---

## OpenACC sistemske promenljive

.center-table.small[

| **Promenljiva** |                                    **Namena**                                    |
|:---------------:|:--------------------------------------------------------------------------------:|
| ACC_DEVICE_TYPE | Tip ureÄ‘aja koji se koristi za pokretanje koda. Opcije su NVIDIA, RADEON, i HOST |
|  ACC_DEVICE_NUM |            ID fiziÄkog akceleratorskog ureÄ‘aja koji se valja koristiti           |
|   ACC_PROFLIB   |                            Biblioteka za profiliranje                            |

]

---

## OpenACC direktive

- Rade neverovatno sliÄno kao u `OpenMP`-u.
- Sve poÄinju sa `#pragma acc`
- Zatim ide direktiva te onda klauzule koje parametrizuju direktivu.

---

## Konstrukt paralelizma

- `#pragma acc parallel`
- Ovo se odnosi na blok koda i znaÄi da se on izvrÅ¡ava u paraleli
- Podrazumevano je da se izvrÅ¡ava u GR (ne-baÅ¡-paralelnom) modu osim ako ne dodamo klauzule koje to spreÄe.
- Svaki paralelni blok se zavrÅ¡ava sa implicitnom sinhronizacijom.
- Paralelni blokovi ne smeju da imaju grananje.

---

## Klauzule `#acc parallel` direktive

- `async`[(integer)]
    - Uklanja implicitnu barijeru na kraju i omoguÄ‡ava da procesor-domaÄ‡in radi stvari konkurentno sa kodom na GPU-u. MoÅ¾e da u zagradi ima brojku. Ta brojka je identifikator reda izvrÅ¡avanja (activity queue) koji obraÄ‘uje stavke ovog bloka. Ta brojka se kasnije moÅ¾e koristiti kao parametar 'wait' klauzule da bi se omoguÄ‡ila sinhronizacija.

- `wait` [(integer-list)]
    - Blokira izvrÅ¡avanje dok se navedeni redovi izvrÅ¡avanja (kao brojevi u listi parametara) nisu kompletirali. Ako se ne navedu brojevi, Äeka se da se sav asinhroni posao ne zavrÅ¡i.
    
---

## Klauzule #acc parallel direktive

- `num_gangs`(integer)
    - TraÅ¾i (ali ne dobija garantovano) koliko gang-ova Ä‡e se koristiti za podelu posla.
- `num_workers`(integer)
    - Kao gore, ali za radnike. Odnosi se samo na WP mod, naravno.
- `vector_length`(integer)
    - Kao gore, ali traÅ¾i odreÄ‘eni broj vektorskih procesnih linija, u VP reÅ¾imu. Na NVidia ureÄ‘ajima valja birati umnoÅ¾ak 32.

---

## Kernels konstrukt

- `#pragma acc kernels`
- Odnosi se na blok, kao i parallel
- PonaÅ¡anje odreÄ‘uju klauzule
- Radi kao parallel (i prima iste klauzule) sa jednom ogromnom razlikom.
- Tamo gde je parallel zahtevao ruÄno podeÅ¡avanje, kernels konstrukt analizira kod i sam distribuira posao.
- Drugim reÄima ovo je magiÄna `make my code fast` direktiva koja zna da prepozna kada imamo, npr. tri ugnjeÅ¾dena `for loop`-a, da to podeli na `gang/worker/vector` nivou.

---

## Upravljanje podacima

- NajvaÅ¾nije usko grlo u programiranju akceleratora jeste u tome Å¡to je memorija akceleratora odvojena od memorije raÄunara domaÄ‡ina
- Sva komunikacija izmeÄ‘u ove dve memorije prolazi kroz magistralu koja je veÄ‡ preoptereÄ‡ena, skoro po definiciji.
- Ovo nije garantovano ponaÅ¡anje: `AMD` ima ureÄ‘aje za ubraznje proraÄuna koji dele memoriju sa glavnim procesorom, te je overhead minimalan.
- Ali `GPU`-ovi su napravljeni tako kako jesu zbog potreba igara, i stoga, kada jednom uÄitate teksture i Å¡ejdere u radnu memoriju kartice, proraÄuni idu gotovo bez komunikacije sa ostatkom sveta.

---

## Upravljanje podacima

- Automatizam je, u teoriji, moguÄ‡.
- U praksi? 
    - C/C++ je toliko kompleksan da je vrlo teÅ¡ko da statiÄkom analizom doÄ‘emo do zakljuÄka koji deo memorije se kako koristi.
- Podrazumevano, OpenACC greÅ¡i na stranu taÄnog izvrÅ¡avanja na raÄun brzine i kopira sve:
    - Sve strukture podataka se kopiraju sa domaÄ‡ina na karticu
    - Radi se proraÄun
    - Kopiraju se svi podaci nazad.
- Automatsko kopiranje ove vrste samo radi bezbolno i implicitno ako se radi sa statiÄki dimenzionisanim nizovima. 
- DinamiÄki alocirana memorija se kopira rukom.

---

## RuÄno upravljanje memorijom

- MoguÄ‡e je upravljati memorijom direktno kroz klauzule za upravljanje memorijom koje idu posle veÄ‡ine direktiva (`parallel` i `kernel`, recimo).
- Metod rada sa memorijom se bazira na brojanju referenci: svaka struktura memorije ili postoji u memoriji kartice ili ne. Ako postoji, broji se broj blokova u programu koji traÅ¾i pristup toj strukturi kroz broj referenci.
- ÄŒim se zavrÅ¡i blok koji je koristio neku strukturu, broj referenci se smanji za jedan i ako stigne na 0, podaci se kopiraju nazad u glavnu memoriju.

---

## RuÄno upravljanje memorijomâ€” klauzule

.medium[
- `copy(variable-list)`
    - `Copy` klauzula specificira koji podaci trebaju datom paralelnom regionu. Promenljive koje nisu kopirane se kopiraju, a promenljive koje jesu se anotiraju sa poveÄ‡anim brojem reference.
- `copyin(variable-list)`
    - Kao `copy`, ali se podaci kada broj referenci stigne do 0 ne kopiraju nazad, no se samo dealociraju.
- `copyout(variable-list)`
    - Kao `copy`, ali se niÅ¡ta ne prenosi u memoriju. Broj referenci se poveÄ‡ava, i ako je 0 memorija se alocira ali se niÅ¡ta ne prenosi na samu karticu. Kada broj referenci dostigne 0, podaci se kopiraju nazad.
- `create(variable-list)`
    - Kao copy, ali niti prenosi podatke na karticu, niti ih vraÄ‡a nazad.
    - Esencijalno alocira pomoÄ‡nu memoriju na kartici.
]

---

## Specifikacija promenljivih

- `variable-list` se sastoji od promenljivih razdvojenih zarezima
- Promenljiva se sastoji od obaveznog identifikatora i opcionogdimenzionisanja
- Identifikator je ime promenljive
- Dimenzionisanje se sastoji od specifikacije delova strukture (multidimenizionalnog niza) koji se kopiraju i navodi se za svaku dimenziju kao specifikacija raspona
- Raspon se piÅ¡e u uglastim zagradama i sastoji se od poÄetnog indeksa i duÅ¾ine razdvojene dvotaÄkom.
- PoÄetna vrednost se moÅ¾e preskoÄiti (te je onda 0)
- Krajnja vrednost se moÅ¾e preskoÄiti na statiÄki dimenzionisanim nizovima (te je onda ravna duÅ¾ini niza).

---

## Specifikacija promenljivihâ€”primeri

- `a[5:t]`
    - Niz a poÄevÅ¡i od 6-tog elementa sa ukupno t elemenata, tj. `a[5], a[6],â€¦, a[5+t-1]`
- `mat[:N][16:32]`
    - Region matrice mat koji obuhvata prvih N redova i sekcije kolona od po 32 elementa koje poÄinju sa 17-im elementom svakog reda.

---

## Definisanje n-dimenzionalnih nizova u C/C++ na OpenACC-kompatibilan naÄin 1.

1. StatiÄki dimenzionisani nizovi
    - OgraniÄenje: ako radimo sa ovim, naÅ¡e specifikacije onoga Å¡to se prenosi mora da definiÅ¡e kontinualni region memorije.
2. PokazivaÄi na statiÄki dimenzionisane nizove
3. StatiÄki alocirani nizovi pokazivaÄa
4. PokazivaÄi na nizove pokazivaÄa
5. MeÅ¡ane alokacije

---

## OpenACC i petlje: loop pragma

- `#pragma acc loop`
- Ide uvek ispred for petlje
- Mora biti ili kombinovana sa ili unutar parallel/kernel direktive
- Kernel direktiva ih pravi sama, ali moÅ¾emo da mi ubacimo naÅ¡e da kaÅ¾emo sistemu taÄno Å¡ta hoÄ‡emo.
- Kao i obiÄno, klauzule odreÄ‘uju ponaÅ¡anje ove pragme

---

## Klauzule loop pragme

- `collapse`(integer)
    - Na koliko ugnjeÅ¾denih for petlji se odnosi ova direktiva. 
    - Podrazumevano je 1.
- `gang`[(static:integer|*)]
    - Aktivira paralelno izvrÅ¡avanje po gang-ovima
    - StatiÄki parametar nam omoguÄ‡ava da podesimo chunking kao i u OpenMP-u
    - * znaÄi da chunking ostavljamo implementaciji
- `worker`
    - Aktivira paralelizam po worker-ima, tj. WP-mod

---

## Klauzule loop pragme

- `vector`
    - Aktivira VP mod
- `auto`
    - Dobijemo Å¡ta inaÄe radi kernel podrazumevano: statiÄku analizu koda i automatsku paralelizaciju.
- `independent`
    - Garantujemo kompajleru da je svaka iteracija naÅ¡e petlje potpuno nezavisna od svake druge, te da se kod moÅ¾e paralelizovati mnogo viÅ¡e.
- `reduction`(operator:variable[,variableâ€¦])
    - Kao i u svim okruÅ¾enjima do sada, radimo sa redukcijom.
    - Operator moÅ¾e biti `+, *, max, min, /, |, &&, i ||`.
    - Promenljive koje se navode kao uÄesnici ne smeju a budu elementi niza ili strukture. To jest, samo obiÄni skalari.
---

## OpenACC i doseg promenljivih

- OpenACC jako vodi raÄuna o dosegu: jako je bitno gde se promenljive deklariÅ¡u
- Na primer, sve promenljive u petlji su privatne za niti koja izvrÅ¡ava datu iteraciju petlje.
- Promenljive u bloku koji je markiran za VP su privatne za vektorsku liniju izvrÅ¡avanja.
- Promenljive u bloku koji je markiran za WP su privatne za svakog radnika, ali deljene kroz vektorske linije izvrÅ¡avanja.
- MoÅ¾emo da iznudimo ovo ponaÅ¡anje kroz â€™privateâ€™ klauzulu.

---

## Eksplicitna sinhronizacija u OpenACC

- Valja je izbeÄ‡i.
- Ali ponekad mora.
- E, pa, kada mora onda je proces ovakav:
 - `#pragma acc atomic`
- DefiniÅ¡e statement koji je atomski, to jest, ne moÅ¾e se prekinuti.
- Tip atomskog izvrÅ¡avanja se definiÅ¡e klauzulom

---

## Atomske klauzule

- `read`
    - Garantovan atomski pristup promenljivama sa desne strane operatora dodele.
- `write`
    - Garantovan atomski pristup promenljivama sa leve strane operatora dodele. 
- `update`
    - Garantovano i Äitanje i pisanje, ali samo u fiksnim formama koje koriste readmodify-write sekvencu kao Å¡to je prefix i postfix inkrement i dekrement kao i svi operatori forme op=
- `capture`
    - Poseban sluÄaj kada hoÄ‡emo da zaÅ¡titimo update-tip operacije sa desne strane znaka jednako, a sa leve vrednost koja hvata vrednost modifikovane promenljive ili pre ili posle modifikacije tj. a = i++;

--

class: center, middle, theend, hide-text
layout: false
background-image: url(../theend.gif)

</textarea>
	<script src="../remark-latest.min.js"></script>
	<script>
		// https://github.com/gnab/remark/issues/72
		        remark.macros.scale = function (percentage) {
		            var url = this;
		            return '<div class="center"><img src="'
		                 + url + '" style="width: ' + percentage + '" /></div>';
		        };
		        var slideshow = remark.create({
		                    highlightLanguage: 'python',
		                    // highlightStyle: 'obsidian',
		                    highlightStyle: 'github',
		                    highlightLines: true,
		                    countIncrementalSlides: false,
		                    navigation: {
		                      // Enable or disable navigating using scroll
		                      // Default: true
		                      // Alternatives: false
		                      scroll: false,
		
		                      //click: true,
		                    }
		                });
	</script>
	<script src="../mermaid.min.js"></script>
	<script>
		mermaid.initialize({startOnLoad:true});
	</script></body>

</html>