<!DOCTYPE html>
<html>

<head>
	<title>[High-performance computing (HPC)](/courses/#table-of-contents)</title>
	<meta charset="utf-8">
	<link rel="stylesheet" href="../remarkslides.css">
	<!-- MathJax‚Ñ¢ -->
	<script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML" async>
	</script>
	<!-- mermaid dijagram -->
	<link rel="stylesheet2" href="../mermaid.min.css">
	<script>
		mermaid.initialize({startOnLoad:true});
	</script>
	<!-- Global site tag (gtag.js) - Google Analytics -->
	<script async src="https://www.googletagmanager.com/gtag/js?id=UA-127734928-1"></script>
	<script>
		window.dataLayer = window.dataLayer || [];
		      function gtag(){dataLayer.push(arguments);}
		      gtag('js', new Date());
		
		      gtag('config', 'UA-127734928-1');
	</script>
	<!-- google analytics -->
	<script>
		(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
		      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
		      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
		      })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
		      ga('create', 'UA-127734928-1', 'auto');
		      ga('send', 'pageview');
	</script>
</head>

<body>
	<textarea id="source">class: center, middle

## [High-performance computing (HPC)](/courses/#table-of-contents)
### OpenMP


.author[[dr. Milovan Toma≈°eviƒá](https://www.milovantomasevic.com/resume/)]

.small[[Fakulteta za informacijske ≈°tudije v Novem mestu (FI≈†)](https://www.fis.unm.si/en/)</br>![:scale 10%](../fis/fis.png) .small[ [üåê‚ûô milovan.tomasevic.fis.unm.si](http://milovan.tomasevic.fis.unm.si)</br> [üìß‚ûô milovan.tomasevic@fis.unm.si](mailto:milovan.tomasevic@fis.unm.si)]]



.created[08.03.2019 u 18:04]


---


name: content

# Content

### Components of OpenMP:

- [Goal](#goal)
- [Execution model (multithreading)](#openmp)
- [Library functions](#lib)
- [OpenMP directives](#dir)
- [OpenMP worksharing](#work)

---
name: goal
class: center, middle, inverse

# Goal

---
layout: true
.section[[Goal](#content)]

---
## The goal is to learn:

- How multithreading can be used for parallel computation
- The structure of the OpenMP framework (library, directives)
- How OpenMP can be used to distribute work

---

layout: false
name: openmp
class: center, middle, inverse

# Execution model (multithreading)

---
layout: true

.section[[Execution model (multithreading)](#content)]

---
## OpenMP

- OpenMP is for *shared memory* parallelization

![:scale 80%](img/openmp.gif)

---
## Execution model

-  OpenMP Execution model: multithreading to distribute work

![:scale 80%](img/openmpmodel.png)

---

## Multithreading

- Threads may be scheduled on different CPU cores (by OS)
	- exploits multi-core hardware
- Our task as programmers (e.g. with OpenMP) :
	- Create multiple threads
	- Distribute work over threads
	- Make sure threads can execute independently! (dependencies / race conditions)

---
## Multithreading

- Multithreaded process is a single process.
- E.g. inspecting an OpenMP program with 4 threads, using ‚Äòtop‚Äô command:

![:scale 85%](img/multi.png)

---

layout: false
name: lib
class: center, middle, inverse

# Library functions

---
layout: true
.section[[Library functions](#content)]

---

## OpenMP Library routines

- Library routines are normal C/C++/Fortran functions from the OpenMP library.
	- `omp_get_thread_num`: returns number of current thread
	- `omp_get_num_threads`: returns total number of threads
	- `omp_get_wtime`: return elapsed walltime in seconds
	- Many more (search web: OpenMP Cheat Sheet)

---

## Exercise: OpenMP Library Routine

- Exercises in this course will be in C/C++, but note that OpenMP also supports Fortran

---

## C/C++ program structure

![:scale 85%](img/openmpe.png)

--

![:scale 85%](img/openmpe1.png)

--

![:scale 85%](img/openmpe2.png)

---

## Let‚Äôs try! Compile & run using

.message.is-info[
.message-header[
Exercise
]
.message-body[
- <a target="_blank" rel="noopener noreferrer" href="/courses/openmp-training/#table-of-contents"> ‚òõ `omp_threadnum.c`</a>
```console
gcc -o omp_threadnum omp_threadnum.c -fopenmp
./omp_threadnum
```

![:scale 70%](img/openmpe3.png)

- Result:
```sh
mt@mt:~/OpenMP/exercises$ ./omp_threadnum 
Hello World from thread = 0
```
]
]

- Only one thread! For now...

---

layout: false
name: dir
class: center, middle, inverse

# OpenMP directives

---
layout: true
.section[[OpenMP directives](#content)]

---
## OpenMP directives

- OpenMP uses directives to create threads. These have the general form

```c
	#pragma omp directive-name
```

- For example, the `parallel` directive can be used to create threads, and execute the following code section in parallel

```c
	#pragma omp parallel
	{
		omp_get_thread_num();
	}
```

---
## Exercise: OpenMP parallel ‚ÄúHello world‚Äù

.message.is-info[
.message-header[
Exercise
]
.message-body[
- Edit `omp_hello.c` to put the `printf` in a parallel section:

```c
#pragma omp parallel
	{
		...
	}
```

- Compile and run: <a target="_blank" rel="noopener noreferrer" href="/courses/openmp-training/#table-of-contents"> ‚òõ `omp_hello.c`</a>
```console
gcc -o omp_hello omp_hello.c -fopenmp
./omp_hello
```
]
]

---
## Exercise: OpenMP parallel ‚ÄúHello world‚Äù

.message.is-success[
.message-header[
Solution
]
.message-body[
![:scale 55%](img/openmpe5.png)

- Result:
.medium[
```sh
mt@mt:~/OpenMP/solutions$ gcc -o omp_hello omp_hello.c -fopenmp
mt@mt:~/OpenMP/solutions$ ./omp_hello 
Hello World from thread = 0
Hello World from thread = 6
Hello World from thread = 4
Hello World from thread = 3
Hello World from thread = 5
Hello World from thread = 2
Hello World from thread = 1
Hello World from thread = 7
```
]
- Random, order, why?
]
]

---
## Private and shared variables

- *Private variables*: each thread works on it‚Äôs own copy of the variable
- *Shared variables*: threads work on a single copy of the variable
- By default, most OpenMP variables are `shared` variables

---
## OpenMP clauses for directives

- OpenMP directives can have `clauses`
```c
#pragma omp directive-name [clause]
```
- Clauses are like additional arguments to the directive. E.g.

```c
	#pragma omp parallel private(x)
	{
		...
	}
```

- Will make sure each thread in the parallel section works on its own copy of ‚Äòx‚Äô

---
## Private and shared variables

- Slightly different ‚ÄúHello world‚Äù: what can go wrong? Which variables (should) be made private?

![:scale 70%](img/openmpe7.png)

---
## Private and shared variables

- The risk: race conditions

![:scale 70%](img/openmpe8.png)

---
## Private and shared variables

- Solution: `private variable`. 
- Each thread should have its own copy of `Tid`

![:scale 70%](img/openmpe9.png)

---
## Private and shared variables

- Each thread should have it‚Äôs own `tid`. 
- Use the `private` clause!

![:scale 70%](img/openmpe10.png)

---

layout: false
name: work
class: center, middle, inverse

# OpenMP Worksharing

---
layout: true
.section[[OpenMP Worksharing](#content)]

---
## OpenMP Worksharing

- You could use thread IDs to distribute work, but OpenMP offers more convenient mechanisms.
- Example: `parallel for-loop`

![:scale 70%](img/openmpe11.png)

---
## OpenMP Worksharing

- You could use thread IDs to distribute work, but OpenMP offers more convenient mechanisms.
- Example: `parallel for-loop`

![:scale 70%](img/openmpe12.png)

---
## Exercise: OpenMP Worksharing

.message.is-info[
.message-header[
Exercise
]
.message-body[

- The file <a target="_blank" rel="noopener noreferrer" href="/courses/openmp-training/#table-of-contents"> ‚òõ `omp_vectadd.c`</a> contains code that:
	- Generating two random vectors with 1M elements
	- Adds the two vectors in a `for-loop`
- Exercise:
	- Add the OpenMP `parallel for` directive to parallelize the loop
	- Compile the program:
	```sh
gcc -o omp_vectadd omp_vectadd.c -fopenmp
```
	- Run the program in serial & parallel:
	```sh
		OMP_NUM_THREADS=1 ./omp_vectadd
		OMP_NUM_THREADS=4 ./omp_vectadd
	```

- N.B. the `OMP_NUM_THREADS` environment variable controls the max. nr. of threads created by OpenMP
]
]

---
## Exercise: OpenMP Worksharing

.message.is-warning[
.message-header[
Info
]
.message-body[
- Which takes longer?
	- `OMP_NUM_THREADS=1 ./omp_vectadd`
	- `OMP_NUM_THREADS=4 ./omp_vectadd`
- Why?
- Overhead is significant if the computation `inside` the parallel region is `light`!
]
]

---
## Reduction clause

- You may want to parallelize a sum, e.g.
```c
#pragma omp parallel for
for(i=0; i<100; i++)
{sum = sum+i;}
```

- But `sum = sum+i` creates a race condition: all threads reading and writing to the same variable!

---
## Reduction clause

- Ok, what about...
```c
#pragma omp parallel for private(sum)
for(i=0; i<100; i++)
{sum = sum+i;}
```
- ?
- Declaring `sum` private doesn‚Äôt help: you need to add all `private` sums at the end.

---

## Reduction clause

- This is where the reduction clause comes in:
```c
#pragma omp parallel for reduction([reduction-op],[init_value])
```

- Example:
```c
#pragma omp parallel for reduction(+,sum)
for(i=0; i<100; i++)
{sum = sum+i;}
```

- Other reduction-ops: `+, -, *, max, min, &, &&, |, ||, ^`

---
## Exercise: Calculate PI

.message.is-info[
.message-header[
Exercise
]
.message-body[
- Exercise: PI can be approximated using Leibnitz sum.
	- Adept <a target="_blank" rel="noopener noreferrer" href="/courses/openmp-training/#table-of-contents"> ‚òõ `omp_pi.c`</a> to parallelize the for loop calculating pi
	- Use the reduction clause `(#pragma omp parallel for reduction(+,pi))`
	- Compile with 
		```sh
			gcc -o omp_pi omp_pi.c -fopenmp
		```
	- Run with 
```console
		OMP_NUM_THREADS=1 ./omp_pi
		OMP_NUM_THREADS=4 ./omp_pi
		OMP_NUM_THREADS=24 ./omp_pi
```
	- What is different from the vector addition in terms of speed?
	- How far does the problem scale?
	- Increasing (only) `OMP_NUM_THREADS`, is that strong or weak scaling?

]
]

---
layout:false

# OpenMP summary

- OpenMP...
	- uses multithreading
	- consists of `library routines, directives`
	- enables easy distribution of work through work sharing directives
	- directives have optional clauses (`arguments`)
	- the reduction clause can be used to parallelize recursive sums, products, max/min opreations etc.
	- has an excellent cheat sheet!

---

name: end
class: center, middle

# That's All üë®üèª‚Äçüéì
# Thank you for listening!

--

class: center, middle, theend, hide-text
layout: false
background-image: url(../theend.gif)

</textarea>
	<script src="../remark-latest.min.js"></script>
	<script>
		// https://github.com/gnab/remark/issues/72
		        remark.macros.scale = function (percentage) {
		            var url = this;
		            return '<div class="center"><img src="'
		                 + url + '" style="width: ' + percentage + '" /></div>';
		        };
		        var slideshow = remark.create({
		                    highlightLanguage: 'python',
		                    // highlightStyle: 'obsidian',
		                    highlightStyle: 'github',
		                    highlightLines: true,
		                    countIncrementalSlides: false,
		                    navigation: {
		                      // Enable or disable navigating using scroll
		                      // Default: true
		                      // Alternatives: false
		                      scroll: false,
		
		                      //click: true,
		                    }
		                });
	</script>
	<script src="../mermaid.min.js"></script>
	<script>
		mermaid.initialize({startOnLoad:true});
	</script></body>

</html>